{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1f1jzgzneYsfbpAJFz56mvlR0nXHpRXnQ",
      "authorship_tag": "ABX9TyMFfWyxWmV8Ln6JfpsvZ4AP",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/xohyun/TimeSeries-Anomaly-Detection-Dataset/blob/master/Data_preprocessing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Implementation of preprocessing.\n",
        "References\n",
        "    - https://github.com/imperial-qore/TranAD/blob/main/preprocess.py\n",
        "\"\"\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "esJrC-fT-PaR",
        "outputId": "910903b3-7753-4877-8009-27fd1b7838da"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nImplementation of preprocessing.\\nReferences\\n    - https://github.com/imperial-qore/TranAD/blob/main/preprocess.py\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Drive mount"
      ],
      "metadata": {
        "id": "sqk70vS38nR1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GPAGhPm-b0Ca",
        "outputId": "f0759aba-cce3-46d3-a7dd-ddd511d0167c"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Import"
      ],
      "metadata": {
        "id": "64ZIF_Wz8sBu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import shutil\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import matplotlib.pyplot as plt\n",
        "import json\n",
        "import datetime"
      ],
      "metadata": {
        "id": "rBYpbJKuch4N"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "zvZDD8n3btYj"
      },
      "outputs": [],
      "source": [
        "def create_folder(directory):\n",
        "    '''\n",
        "    Create folder\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    directory : path and directory that you want to create\n",
        "    \n",
        "    Returns\n",
        "    -------\n",
        "    None\n",
        "    '''\n",
        "    try:\n",
        "        if not os.path.exists(directory):\n",
        "            os.makedirs(directory)\n",
        "    except OSError:\n",
        "        print ('Error: Creating directory ' +  directory)\n",
        "\n",
        "def prepare_data(dataset, *args, **kwargs):\n",
        "    '''\n",
        "    Preprocessing function + save dataset as numpy file\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    dataset : dataset that you want to preprocess (WADI, MSL, SMAP, NAB, SMD)\n",
        "    \n",
        "    Returns\n",
        "    -------\n",
        "    None\n",
        "    '''\n",
        "    if dataset == 'WADI':\n",
        "        dataset_folder = kwargs['dataset_folder']\n",
        "        output_folder = '/content/data/WADI'\n",
        "        create_folder(output_folder)\n",
        "\n",
        "        train = pd.read_csv(os.path.join(dataset_folder, 'WADI_14days_new.csv'))\n",
        "        test = pd.read_csv(os.path.join(dataset_folder, 'WADI_attackdataLABLE.csv'), header=0, low_memory=False, index_col=0)\n",
        "        test = test.rename(columns=test.iloc[0]).iloc[1:].reset_index() # replace first row with column name\n",
        "\n",
        "        label = test[['Attack LABLE (1:No Attack, -1:Attack)']]\n",
        "        test = test.drop(['Attack LABLE (1:No Attack, -1:Attack)'], axis='columns')\n",
        "\n",
        "        row = test[test.isna().all(axis=1)].index.tolist() # find the index of the row in which all columns are NA\n",
        "        label = label.drop(row, axis=0).replace(['-1', '1'], [1, 0]) # 1 : anomaly\n",
        "\n",
        "        train = train.dropna(how='all', axis=0) #inplace=True\n",
        "        test = test.dropna(how='all', axis=0) #inplace=True\n",
        "\n",
        "        col = train.columns[train.isna().any()] # find the index of the columns in which rows are NA\n",
        "        train = train.dropna(axis=1)\n",
        "        test = test.drop(col, axis=1)\n",
        "\n",
        "        print(f\"train shape:{train.shape}, test shape :{test.shape}, label shape:{label.shape}\")\n",
        "\n",
        "        #---# To save train data #---#\n",
        "        for i in set(train['Date']):\n",
        "            if len(train[train['Date'] == i]) % 60 != 0:\n",
        "                print(f\"Exclude {i} data\")\n",
        "                continue\n",
        "            df_train = train[train['Date'] == i]\n",
        "            \n",
        "            start = pd.to_datetime(df_train.iloc[0]['Date'] + ' ' + df_train.iloc[0]['Time'])    \n",
        "            date_range = pd.date_range(start = start, periods=len(df_train), freq='s')\n",
        "            df_train = df_train.drop(['Date', 'Time'], axis = 1)\n",
        "            df_train.insert(1, 'Time', date_range)\n",
        "            \n",
        "            #---# MinMaxScaler #---#\n",
        "            values = df_train.iloc[:, 2:] # Exclude the time and index columns\n",
        "            scaler = MinMaxScaler()\n",
        "            values = scaler.fit_transform(values)\n",
        "            df_train.iloc[:, 2:] = values\n",
        "\n",
        "            date = i.replace('/', '_')\n",
        "            np.save(os.path.join(output_folder, f\"{date}_train.npy\"), df_train.to_numpy())\n",
        "\n",
        "        #---# To save test data, label data #---#\n",
        "        for i in set(test['Date ']):\n",
        "            df_test = test[test['Date '] == i]\n",
        "            df_label = label[test['Date '] == i]\n",
        "            \n",
        "            start = pd.to_datetime(df_test.iloc[0]['Date '] + ' ' + df_test.iloc[0]['Time'])\n",
        "            # end = start + datetime.timedelta(hours=int(len(df_test) / (60*60)))\n",
        "            # pd.date_range(start = start, end = end, freq='1s')\n",
        "            \n",
        "            date_range = pd.date_range(start = start, periods=len(df_test), freq='s')\n",
        "            df_test = df_test.drop(['Date ', 'Time'], axis = 1)\n",
        "            df_test.insert(1, 'Time', date_range)\n",
        "\n",
        "            #---# MinMaxScaler #---#\n",
        "            values = df_test.iloc[:, 2:] # Exclude the time and index columns\n",
        "            scaler = MinMaxScaler()\n",
        "            values = scaler.fit_transform(values)\n",
        "            df_test.iloc[:, 2:] = values\n",
        "\n",
        "            date = i.replace('/', '_')\n",
        "            np.save(os.path.join(output_folder, f\"{date}_test.npy\"), df_test.to_numpy())\n",
        "            np.save(os.path.join(output_folder, f\"{date}_labels.npy\"), df_label.to_numpy())\n",
        "\n",
        "    elif dataset == 'MSL' or dataset == 'SMAP':\n",
        "        # choose_data = ['A-4', 'C-2', 'T-1']\n",
        "\n",
        "        dataset_folder = '/content/original_data/SMAP_MSL'\n",
        "        output_folder = os.path.join('/content/data', dataset)\n",
        "        create_folder(output_folder)\n",
        "\n",
        "        file_ = os.path.join(dataset_folder, 'labeled_anomalies.csv')\n",
        "        values = pd.read_csv(file_)\n",
        "\n",
        "        values = values[values['spacecraft'] == dataset]\n",
        "        filenames = values['chan_id'].values.tolist()    \n",
        "\n",
        "        for fn in filenames:\n",
        "            # if fn not in choose_data:\n",
        "            #     continue\n",
        "            train = np.load(f'{dataset_folder}/train/{fn}.npy')\n",
        "            test = np.load(f'{dataset_folder}/test/{fn}.npy')\n",
        "\n",
        "            #---# MinMaxScaler #---#\n",
        "            scaler = MinMaxScaler()\n",
        "            train = scaler.fit_transform(train)\n",
        "            test = scaler.transform(test)\n",
        "            \n",
        "            #---# save train.npy and test.npy #---#\n",
        "            np.save(f'{output_folder}/{fn}_train.npy', train)\n",
        "            np.save(f'{output_folder}/{fn}_test.npy', test)\n",
        "\n",
        "            #---# save labels.npy #---#\n",
        "            labels = np.zeros(test.shape)\n",
        "            indices = values[values['chan_id'] == fn]['anomaly_sequences'].values[0]\n",
        "            indices = indices.replace(']', '').replace('[', '').split(', ')\n",
        "            indices = [int(i) for i in indices]\n",
        "            for i in range(0, len(indices), 2):\n",
        "                labels[indices[i]:indices[i+1], :] = 1\n",
        "            np.save(f'{output_folder}/{fn}_labels.npy', labels)\n",
        "  \n",
        "    elif dataset == 'NAB':\n",
        "        dataset_folder = 'original_data/NAB/realKnownCause'\n",
        "        label_folder = 'original_data/NAB/labels'\n",
        "        output_folder = 'data/NAB'\n",
        "        create_folder(output_folder)\n",
        "\n",
        "        file_list = os.listdir(dataset_folder)\n",
        "\n",
        "        with open(label_folder + '/combined_windows.json') as f:\n",
        "            labeldict = json.load(f)\n",
        "\n",
        "        for filename in file_list:\n",
        "            if not filename.endswith('.csv'): continue\n",
        "            df = pd.read_csv(dataset_folder+'/'+filename)\n",
        "        \n",
        "            print(f\"{filename} shape {df.shape}\")\n",
        "            values = df.values[:,1]\n",
        "            \n",
        "            #---# MinMaxScaler #---#\n",
        "            scaler = MinMaxScaler()\n",
        "            values = scaler.fit_transform(values.reshape(-1,1))\n",
        "\n",
        "            #---# Label #---#\n",
        "            labels = np.zeros_like(values, dtype=np.float64)\n",
        "            for timestamp in labeldict['realKnownCause/'+filename]:\n",
        "                tstamp = timestamp[0].replace('.000000', '')\n",
        "                start_index = np.where(((df['timestamp'] == tstamp).values + 0) == 1)[0][0]\n",
        "                tstamp = timestamp[1].replace('.000000', '')\n",
        "                end_index = np.where(((df['timestamp'] == tstamp).values + 0) == 1)[0][0]\n",
        "                labels[start_index : end_index] = 1\n",
        "\n",
        "            #---# Split train npy and test npy #---#\n",
        "            train, test = values, values\n",
        "            train, test, labels = train.reshape(-1, 1), test.reshape(-1, 1), labels.reshape(-1, 1)\n",
        "            \n",
        "            #---# Save file #---#\n",
        "            fn = filename.replace('.csv', '')\n",
        "            for file in ['train', 'test', 'labels']:\n",
        "                np.save(os.path.join(output_folder, f'{fn}_{file}.npy'), eval(file))\n",
        "\n",
        "    elif dataset == 'SMD':\n",
        "        dataset_folder = '/content/original_data/SMD'\n",
        "        output_folder = '/content/data/SMD'\n",
        "        create_folder(output_folder)\n",
        "\n",
        "        file_list = os.listdir(os.path.join(dataset_folder, \"train\"))\n",
        "        for filename in file_list:\n",
        "            if filename.endswith('.txt'):\n",
        "                #---# train #---#\n",
        "                values_train = np.genfromtxt(os.path.join(dataset_folder, 'train', filename), delimiter=',')\n",
        "                scaler = MinMaxScaler()\n",
        "                values_train_scale = scaler.fit_transform(values_train)\n",
        "                np.save(os.path.join(output_folder, f\"train_{filename}.npy\"), values_train_scale)\n",
        "                \n",
        "                #---# test #---#\n",
        "                values_test = np.genfromtxt(os.path.join(dataset_folder, 'test', filename), delimiter=',')\n",
        "                scaler = MinMaxScaler()\n",
        "                values_test_scale = scaler.fit_transform(values_train)\n",
        "                np.save(os.path.join(output_folder, f\"test_{filename}.npy\"), values_test_scale)\n",
        "\n",
        "                #---# label #---#\n",
        "                values_label = np.genfromtxt(os.path.join(dataset_folder, 'test_label', filename), delimiter=',')\n",
        "                np.save(os.path.join(output_folder, f\"label_{filename}.npy\"), values_label)\n",
        "\n",
        "                #---# interpretation_label #---#\n",
        "                temp = np.zeros(values_test.shape)\n",
        "                with open(os.path.join(dataset_folder, 'interpretation_label', filename), \"r\") as f:\n",
        "                    ls = f.readlines()\n",
        "                    for line in ls:\n",
        "                        pos, value = line.split(':')[0], line.split(':')[1].split(',')\n",
        "                        start, end, inx = int(pos.split('-')[0]), int(pos.split('-')[1]), [int(i)-1 for i in value]\n",
        "                        temp[start-1:end-1, inx] = 1\n",
        "                        np.save(os.path.join(output_folder, f\"label_{filename}_interpret.npy\"), temp)\n",
        "\n",
        "    else:\n",
        "        print(\"Check the dataset!!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## SMAP & MSL"
      ],
      "metadata": {
        "id": "BSCVyTzciwHF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://s3-us-west-2.amazonaws.com/telemanom/data.zip && unzip data.zip && rm data.zip\n",
        "!cd data && wget https://raw.githubusercontent.com/khundman/telemanom/master/labeled_anomalies.csv\n",
        "!mkdir original_data\n",
        "os.rename('/content/data', '/content/SMAP_MSL')\n",
        "shutil.move('/content/SMAP_MSL', '/content/original_data/SMAP_MSL')\n",
        "\n",
        "dataset = 'SMAP'\n",
        "prepare_data(dataset)\n",
        "\n",
        "dataset = 'MSL'\n",
        "prepare_data(dataset)"
      ],
      "metadata": {
        "id": "ZkVtxnvYgQLg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "efe873ef-1c01-4fd8-8417-8e20408e6774"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-01-09 08:17:45--  https://s3-us-west-2.amazonaws.com/telemanom/data.zip\n",
            "Resolving s3-us-west-2.amazonaws.com (s3-us-west-2.amazonaws.com)... 52.92.144.136, 52.218.152.56, 52.218.133.136, ...\n",
            "Connecting to s3-us-west-2.amazonaws.com (s3-us-west-2.amazonaws.com)|52.92.144.136|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 85899803 (82M) [application/zip]\n",
            "Saving to: ‘data.zip’\n",
            "\n",
            "data.zip            100%[===================>]  81.92M  28.3MB/s    in 2.9s    \n",
            "\n",
            "2023-01-09 08:17:48 (28.3 MB/s) - ‘data.zip’ saved [85899803/85899803]\n",
            "\n",
            "Archive:  data.zip\n",
            "   creating: data/2018-05-19_15.00.10/\n",
            "   creating: data/2018-05-19_15.00.10/models/\n",
            "  inflating: data/2018-05-19_15.00.10/models/A-1.h5  \n",
            "  inflating: data/2018-05-19_15.00.10/models/A-2.h5  \n",
            "  inflating: data/2018-05-19_15.00.10/models/A-3.h5  \n",
            "  inflating: data/2018-05-19_15.00.10/models/A-4.h5  \n",
            "  inflating: data/2018-05-19_15.00.10/models/A-5.h5  \n",
            "  inflating: data/2018-05-19_15.00.10/models/A-6.h5  \n",
            "  inflating: data/2018-05-19_15.00.10/models/A-7.h5  \n",
            "  inflating: data/2018-05-19_15.00.10/models/A-8.h5  \n",
            "  inflating: data/2018-05-19_15.00.10/models/A-9.h5  \n",
            "  inflating: data/2018-05-19_15.00.10/models/B-1.h5  \n",
            "  inflating: data/2018-05-19_15.00.10/models/C-1.h5  \n",
            "  inflating: data/2018-05-19_15.00.10/models/C-2.h5  \n",
            "  inflating: data/2018-05-19_15.00.10/models/D-1.h5  \n",
            "  inflating: data/2018-05-19_15.00.10/models/D-11.h5  \n",
            "  inflating: data/2018-05-19_15.00.10/models/D-12.h5  \n",
            "  inflating: data/2018-05-19_15.00.10/models/D-13.h5  \n",
            "  inflating: data/2018-05-19_15.00.10/models/D-14.h5  \n",
            "  inflating: data/2018-05-19_15.00.10/models/D-15.h5  \n",
            "  inflating: data/2018-05-19_15.00.10/models/D-16.h5  \n",
            "  inflating: data/2018-05-19_15.00.10/models/D-2.h5  \n",
            "  inflating: data/2018-05-19_15.00.10/models/D-3.h5  \n",
            "  inflating: data/2018-05-19_15.00.10/models/D-4.h5  \n",
            "  inflating: data/2018-05-19_15.00.10/models/D-5.h5  \n",
            "  inflating: data/2018-05-19_15.00.10/models/D-6.h5  \n",
            "  inflating: data/2018-05-19_15.00.10/models/D-7.h5  \n",
            "  inflating: data/2018-05-19_15.00.10/models/D-8.h5  \n",
            "  inflating: data/2018-05-19_15.00.10/models/D-9.h5  \n",
            "  inflating: data/2018-05-19_15.00.10/models/E-1.h5  \n",
            "  inflating: data/2018-05-19_15.00.10/models/E-10.h5  \n",
            "  inflating: data/2018-05-19_15.00.10/models/E-11.h5  \n",
            "  inflating: data/2018-05-19_15.00.10/models/E-12.h5  \n",
            "  inflating: data/2018-05-19_15.00.10/models/E-13.h5  \n",
            "  inflating: data/2018-05-19_15.00.10/models/E-2.h5  \n",
            "  inflating: data/2018-05-19_15.00.10/models/E-3.h5  \n",
            "  inflating: data/2018-05-19_15.00.10/models/E-4.h5  \n",
            "  inflating: data/2018-05-19_15.00.10/models/E-5.h5  \n",
            "  inflating: data/2018-05-19_15.00.10/models/E-6.h5  \n",
            "  inflating: data/2018-05-19_15.00.10/models/E-7.h5  \n",
            "  inflating: data/2018-05-19_15.00.10/models/E-8.h5  \n",
            "  inflating: data/2018-05-19_15.00.10/models/E-9.h5  \n",
            "  inflating: data/2018-05-19_15.00.10/models/F-1.h5  \n",
            "  inflating: data/2018-05-19_15.00.10/models/F-2.h5  \n",
            "  inflating: data/2018-05-19_15.00.10/models/F-3.h5  \n",
            "  inflating: data/2018-05-19_15.00.10/models/F-4.h5  \n",
            "  inflating: data/2018-05-19_15.00.10/models/F-5.h5  \n",
            "  inflating: data/2018-05-19_15.00.10/models/F-7.h5  \n",
            "  inflating: data/2018-05-19_15.00.10/models/F-8.h5  \n",
            "  inflating: data/2018-05-19_15.00.10/models/G-1.h5  \n",
            "  inflating: data/2018-05-19_15.00.10/models/G-2.h5  \n",
            "  inflating: data/2018-05-19_15.00.10/models/G-3.h5  \n",
            "  inflating: data/2018-05-19_15.00.10/models/G-4.h5  \n",
            "  inflating: data/2018-05-19_15.00.10/models/G-6.h5  \n",
            "  inflating: data/2018-05-19_15.00.10/models/G-7.h5  \n",
            "  inflating: data/2018-05-19_15.00.10/models/M-1.h5  \n",
            "  inflating: data/2018-05-19_15.00.10/models/M-2.h5  \n",
            "  inflating: data/2018-05-19_15.00.10/models/M-3.h5  \n",
            "  inflating: data/2018-05-19_15.00.10/models/M-4.h5  \n",
            "  inflating: data/2018-05-19_15.00.10/models/M-5.h5  \n",
            "  inflating: data/2018-05-19_15.00.10/models/M-6.h5  \n",
            "  inflating: data/2018-05-19_15.00.10/models/M-7.h5  \n",
            "  inflating: data/2018-05-19_15.00.10/models/P-1.h5  \n",
            "  inflating: data/2018-05-19_15.00.10/models/P-10.h5  \n",
            "  inflating: data/2018-05-19_15.00.10/models/P-11.h5  \n",
            "  inflating: data/2018-05-19_15.00.10/models/P-14.h5  \n",
            "  inflating: data/2018-05-19_15.00.10/models/P-15.h5  \n",
            "  inflating: data/2018-05-19_15.00.10/models/P-2.h5  \n",
            "  inflating: data/2018-05-19_15.00.10/models/P-3.h5  \n",
            "  inflating: data/2018-05-19_15.00.10/models/P-4.h5  \n",
            "  inflating: data/2018-05-19_15.00.10/models/P-7.h5  \n",
            "  inflating: data/2018-05-19_15.00.10/models/R-1.h5  \n",
            "  inflating: data/2018-05-19_15.00.10/models/S-1.h5  \n",
            "  inflating: data/2018-05-19_15.00.10/models/S-2.h5  \n",
            "  inflating: data/2018-05-19_15.00.10/models/T-1.h5  \n",
            "  inflating: data/2018-05-19_15.00.10/models/T-10.h5  \n",
            "  inflating: data/2018-05-19_15.00.10/models/T-12.h5  \n",
            "  inflating: data/2018-05-19_15.00.10/models/T-13.h5  \n",
            "  inflating: data/2018-05-19_15.00.10/models/T-2.h5  \n",
            "  inflating: data/2018-05-19_15.00.10/models/T-3.h5  \n",
            "  inflating: data/2018-05-19_15.00.10/models/T-4.h5  \n",
            "  inflating: data/2018-05-19_15.00.10/models/T-5.h5  \n",
            "  inflating: data/2018-05-19_15.00.10/models/T-8.h5  \n",
            "  inflating: data/2018-05-19_15.00.10/models/T-9.h5  \n",
            "  inflating: data/2018-05-19_15.00.10/params.log  \n",
            "   creating: data/2018-05-19_15.00.10/smoothed_errors/\n",
            "  inflating: data/2018-05-19_15.00.10/smoothed_errors/A-1.npy  \n",
            "  inflating: data/2018-05-19_15.00.10/smoothed_errors/A-2.npy  \n",
            "  inflating: data/2018-05-19_15.00.10/smoothed_errors/A-3.npy  \n",
            "  inflating: data/2018-05-19_15.00.10/smoothed_errors/A-4.npy  \n",
            "  inflating: data/2018-05-19_15.00.10/smoothed_errors/A-5.npy  \n",
            "  inflating: data/2018-05-19_15.00.10/smoothed_errors/A-6.npy  \n",
            "  inflating: data/2018-05-19_15.00.10/smoothed_errors/A-7.npy  \n",
            "  inflating: data/2018-05-19_15.00.10/smoothed_errors/A-8.npy  \n",
            "  inflating: data/2018-05-19_15.00.10/smoothed_errors/A-9.npy  \n",
            "  inflating: data/2018-05-19_15.00.10/smoothed_errors/B-1.npy  \n",
            "  inflating: data/2018-05-19_15.00.10/smoothed_errors/C-1.npy  \n",
            "  inflating: data/2018-05-19_15.00.10/smoothed_errors/C-2.npy  \n",
            "  inflating: data/2018-05-19_15.00.10/smoothed_errors/D-1.npy  \n",
            "  inflating: data/2018-05-19_15.00.10/smoothed_errors/D-11.npy  \n",
            "  inflating: data/2018-05-19_15.00.10/smoothed_errors/D-12.npy  \n",
            "  inflating: data/2018-05-19_15.00.10/smoothed_errors/D-13.npy  \n",
            "  inflating: data/2018-05-19_15.00.10/smoothed_errors/D-14.npy  \n",
            "  inflating: data/2018-05-19_15.00.10/smoothed_errors/D-15.npy  \n",
            "  inflating: data/2018-05-19_15.00.10/smoothed_errors/D-16.npy  \n",
            "  inflating: data/2018-05-19_15.00.10/smoothed_errors/D-2.npy  \n",
            "  inflating: data/2018-05-19_15.00.10/smoothed_errors/D-3.npy  \n",
            "  inflating: data/2018-05-19_15.00.10/smoothed_errors/D-4.npy  \n",
            "  inflating: data/2018-05-19_15.00.10/smoothed_errors/D-5.npy  \n",
            "  inflating: data/2018-05-19_15.00.10/smoothed_errors/D-6.npy  \n",
            "  inflating: data/2018-05-19_15.00.10/smoothed_errors/D-7.npy  \n",
            "  inflating: data/2018-05-19_15.00.10/smoothed_errors/D-8.npy  \n",
            "  inflating: data/2018-05-19_15.00.10/smoothed_errors/D-9.npy  \n",
            "  inflating: data/2018-05-19_15.00.10/smoothed_errors/E-1.npy  \n",
            "  inflating: data/2018-05-19_15.00.10/smoothed_errors/E-10.npy  \n",
            "  inflating: data/2018-05-19_15.00.10/smoothed_errors/E-11.npy  \n",
            "  inflating: data/2018-05-19_15.00.10/smoothed_errors/E-12.npy  \n",
            "  inflating: data/2018-05-19_15.00.10/smoothed_errors/E-13.npy  \n",
            "  inflating: data/2018-05-19_15.00.10/smoothed_errors/E-2.npy  \n",
            "  inflating: data/2018-05-19_15.00.10/smoothed_errors/E-3.npy  \n",
            "  inflating: data/2018-05-19_15.00.10/smoothed_errors/E-4.npy  \n",
            "  inflating: data/2018-05-19_15.00.10/smoothed_errors/E-5.npy  \n",
            "  inflating: data/2018-05-19_15.00.10/smoothed_errors/E-6.npy  \n",
            "  inflating: data/2018-05-19_15.00.10/smoothed_errors/E-7.npy  \n",
            "  inflating: data/2018-05-19_15.00.10/smoothed_errors/E-8.npy  \n",
            "  inflating: data/2018-05-19_15.00.10/smoothed_errors/E-9.npy  \n",
            "  inflating: data/2018-05-19_15.00.10/smoothed_errors/F-1.npy  \n",
            "  inflating: data/2018-05-19_15.00.10/smoothed_errors/F-2.npy  \n",
            "  inflating: data/2018-05-19_15.00.10/smoothed_errors/F-3.npy  \n",
            "  inflating: data/2018-05-19_15.00.10/smoothed_errors/F-4.npy  \n",
            "  inflating: data/2018-05-19_15.00.10/smoothed_errors/F-5.npy  \n",
            "  inflating: data/2018-05-19_15.00.10/smoothed_errors/F-7.npy  \n",
            "  inflating: data/2018-05-19_15.00.10/smoothed_errors/F-8.npy  \n",
            "  inflating: data/2018-05-19_15.00.10/smoothed_errors/G-1.npy  \n",
            "  inflating: data/2018-05-19_15.00.10/smoothed_errors/G-2.npy  \n",
            "  inflating: data/2018-05-19_15.00.10/smoothed_errors/G-3.npy  \n",
            "  inflating: data/2018-05-19_15.00.10/smoothed_errors/G-4.npy  \n",
            "  inflating: data/2018-05-19_15.00.10/smoothed_errors/G-6.npy  \n",
            "  inflating: data/2018-05-19_15.00.10/smoothed_errors/G-7.npy  \n",
            "  inflating: data/2018-05-19_15.00.10/smoothed_errors/M-1.npy  \n",
            "  inflating: data/2018-05-19_15.00.10/smoothed_errors/M-2.npy  \n",
            "  inflating: data/2018-05-19_15.00.10/smoothed_errors/M-3.npy  \n",
            "  inflating: data/2018-05-19_15.00.10/smoothed_errors/M-4.npy  \n",
            "  inflating: data/2018-05-19_15.00.10/smoothed_errors/M-5.npy  \n",
            "  inflating: data/2018-05-19_15.00.10/smoothed_errors/M-6.npy  \n",
            "  inflating: data/2018-05-19_15.00.10/smoothed_errors/M-7.npy  \n",
            "  inflating: data/2018-05-19_15.00.10/smoothed_errors/P-1.npy  \n",
            "  inflating: data/2018-05-19_15.00.10/smoothed_errors/P-10.npy  \n",
            "  inflating: data/2018-05-19_15.00.10/smoothed_errors/P-11.npy  \n",
            "  inflating: data/2018-05-19_15.00.10/smoothed_errors/P-14.npy  \n",
            "  inflating: data/2018-05-19_15.00.10/smoothed_errors/P-15.npy  \n",
            "  inflating: data/2018-05-19_15.00.10/smoothed_errors/P-2.npy  \n",
            "  inflating: data/2018-05-19_15.00.10/smoothed_errors/P-3.npy  \n",
            "  inflating: data/2018-05-19_15.00.10/smoothed_errors/P-4.npy  \n",
            "  inflating: data/2018-05-19_15.00.10/smoothed_errors/P-7.npy  \n",
            "  inflating: data/2018-05-19_15.00.10/smoothed_errors/R-1.npy  \n",
            "  inflating: data/2018-05-19_15.00.10/smoothed_errors/S-1.npy  \n",
            "  inflating: data/2018-05-19_15.00.10/smoothed_errors/S-2.npy  \n",
            "  inflating: data/2018-05-19_15.00.10/smoothed_errors/T-1.npy  \n",
            "  inflating: data/2018-05-19_15.00.10/smoothed_errors/T-10.npy  \n",
            "  inflating: data/2018-05-19_15.00.10/smoothed_errors/T-12.npy  \n",
            "  inflating: data/2018-05-19_15.00.10/smoothed_errors/T-13.npy  \n",
            "  inflating: data/2018-05-19_15.00.10/smoothed_errors/T-2.npy  \n",
            "  inflating: data/2018-05-19_15.00.10/smoothed_errors/T-3.npy  \n",
            "  inflating: data/2018-05-19_15.00.10/smoothed_errors/T-4.npy  \n",
            "  inflating: data/2018-05-19_15.00.10/smoothed_errors/T-5.npy  \n",
            "  inflating: data/2018-05-19_15.00.10/smoothed_errors/T-8.npy  \n",
            "  inflating: data/2018-05-19_15.00.10/smoothed_errors/T-9.npy  \n",
            "   creating: data/2018-05-19_15.00.10/y_hat/\n",
            "  inflating: data/2018-05-19_15.00.10/y_hat/A-1.npy  \n",
            "  inflating: data/2018-05-19_15.00.10/y_hat/A-2.npy  \n",
            "  inflating: data/2018-05-19_15.00.10/y_hat/A-3.npy  \n",
            "  inflating: data/2018-05-19_15.00.10/y_hat/A-4.npy  \n",
            "  inflating: data/2018-05-19_15.00.10/y_hat/A-5.npy  \n",
            "  inflating: data/2018-05-19_15.00.10/y_hat/A-6.npy  \n",
            "  inflating: data/2018-05-19_15.00.10/y_hat/A-7.npy  \n",
            "  inflating: data/2018-05-19_15.00.10/y_hat/A-8.npy  \n",
            "  inflating: data/2018-05-19_15.00.10/y_hat/A-9.npy  \n",
            "  inflating: data/2018-05-19_15.00.10/y_hat/B-1.npy  \n",
            "  inflating: data/2018-05-19_15.00.10/y_hat/C-1.npy  \n",
            "  inflating: data/2018-05-19_15.00.10/y_hat/C-2.npy  \n",
            "  inflating: data/2018-05-19_15.00.10/y_hat/D-1.npy  \n",
            "  inflating: data/2018-05-19_15.00.10/y_hat/D-11.npy  \n",
            "  inflating: data/2018-05-19_15.00.10/y_hat/D-12.npy  \n",
            "  inflating: data/2018-05-19_15.00.10/y_hat/D-13.npy  \n",
            "  inflating: data/2018-05-19_15.00.10/y_hat/D-14.npy  \n",
            "  inflating: data/2018-05-19_15.00.10/y_hat/D-15.npy  \n",
            "  inflating: data/2018-05-19_15.00.10/y_hat/D-16.npy  \n",
            "  inflating: data/2018-05-19_15.00.10/y_hat/D-2.npy  \n",
            "  inflating: data/2018-05-19_15.00.10/y_hat/D-3.npy  \n",
            "  inflating: data/2018-05-19_15.00.10/y_hat/D-4.npy  \n",
            "  inflating: data/2018-05-19_15.00.10/y_hat/D-5.npy  \n",
            "  inflating: data/2018-05-19_15.00.10/y_hat/D-6.npy  \n",
            "  inflating: data/2018-05-19_15.00.10/y_hat/D-7.npy  \n",
            "  inflating: data/2018-05-19_15.00.10/y_hat/D-8.npy  \n",
            "  inflating: data/2018-05-19_15.00.10/y_hat/D-9.npy  \n",
            "  inflating: data/2018-05-19_15.00.10/y_hat/E-1.npy  \n",
            "  inflating: data/2018-05-19_15.00.10/y_hat/E-10.npy  \n",
            "  inflating: data/2018-05-19_15.00.10/y_hat/E-11.npy  \n",
            "  inflating: data/2018-05-19_15.00.10/y_hat/E-12.npy  \n",
            "  inflating: data/2018-05-19_15.00.10/y_hat/E-13.npy  \n",
            "  inflating: data/2018-05-19_15.00.10/y_hat/E-2.npy  \n",
            "  inflating: data/2018-05-19_15.00.10/y_hat/E-3.npy  \n",
            "  inflating: data/2018-05-19_15.00.10/y_hat/E-4.npy  \n",
            "  inflating: data/2018-05-19_15.00.10/y_hat/E-5.npy  \n",
            "  inflating: data/2018-05-19_15.00.10/y_hat/E-6.npy  \n",
            "  inflating: data/2018-05-19_15.00.10/y_hat/E-7.npy  \n",
            "  inflating: data/2018-05-19_15.00.10/y_hat/E-8.npy  \n",
            "  inflating: data/2018-05-19_15.00.10/y_hat/E-9.npy  \n",
            "  inflating: data/2018-05-19_15.00.10/y_hat/F-1.npy  \n",
            "  inflating: data/2018-05-19_15.00.10/y_hat/F-2.npy  \n",
            "  inflating: data/2018-05-19_15.00.10/y_hat/F-3.npy  \n",
            "  inflating: data/2018-05-19_15.00.10/y_hat/F-4.npy  \n",
            "  inflating: data/2018-05-19_15.00.10/y_hat/F-5.npy  \n",
            "  inflating: data/2018-05-19_15.00.10/y_hat/F-7.npy  \n",
            "  inflating: data/2018-05-19_15.00.10/y_hat/F-8.npy  \n",
            "  inflating: data/2018-05-19_15.00.10/y_hat/G-1.npy  \n",
            "  inflating: data/2018-05-19_15.00.10/y_hat/G-2.npy  \n",
            "  inflating: data/2018-05-19_15.00.10/y_hat/G-3.npy  \n",
            "  inflating: data/2018-05-19_15.00.10/y_hat/G-4.npy  \n",
            "  inflating: data/2018-05-19_15.00.10/y_hat/G-6.npy  \n",
            "  inflating: data/2018-05-19_15.00.10/y_hat/G-7.npy  \n",
            "  inflating: data/2018-05-19_15.00.10/y_hat/M-1.npy  \n",
            "  inflating: data/2018-05-19_15.00.10/y_hat/M-2.npy  \n",
            "  inflating: data/2018-05-19_15.00.10/y_hat/M-3.npy  \n",
            "  inflating: data/2018-05-19_15.00.10/y_hat/M-4.npy  \n",
            "  inflating: data/2018-05-19_15.00.10/y_hat/M-5.npy  \n",
            "  inflating: data/2018-05-19_15.00.10/y_hat/M-6.npy  \n",
            "  inflating: data/2018-05-19_15.00.10/y_hat/M-7.npy  \n",
            "  inflating: data/2018-05-19_15.00.10/y_hat/P-1.npy  \n",
            "  inflating: data/2018-05-19_15.00.10/y_hat/P-10.npy  \n",
            "  inflating: data/2018-05-19_15.00.10/y_hat/P-11.npy  \n",
            "  inflating: data/2018-05-19_15.00.10/y_hat/P-14.npy  \n",
            "  inflating: data/2018-05-19_15.00.10/y_hat/P-15.npy  \n",
            "  inflating: data/2018-05-19_15.00.10/y_hat/P-2.npy  \n",
            "  inflating: data/2018-05-19_15.00.10/y_hat/P-3.npy  \n",
            "  inflating: data/2018-05-19_15.00.10/y_hat/P-4.npy  \n",
            "  inflating: data/2018-05-19_15.00.10/y_hat/P-7.npy  \n",
            "  inflating: data/2018-05-19_15.00.10/y_hat/R-1.npy  \n",
            "  inflating: data/2018-05-19_15.00.10/y_hat/S-1.npy  \n",
            "  inflating: data/2018-05-19_15.00.10/y_hat/S-2.npy  \n",
            "  inflating: data/2018-05-19_15.00.10/y_hat/T-1.npy  \n",
            "  inflating: data/2018-05-19_15.00.10/y_hat/T-10.npy  \n",
            "  inflating: data/2018-05-19_15.00.10/y_hat/T-12.npy  \n",
            "  inflating: data/2018-05-19_15.00.10/y_hat/T-13.npy  \n",
            "  inflating: data/2018-05-19_15.00.10/y_hat/T-2.npy  \n",
            "  inflating: data/2018-05-19_15.00.10/y_hat/T-3.npy  \n",
            "  inflating: data/2018-05-19_15.00.10/y_hat/T-4.npy  \n",
            "  inflating: data/2018-05-19_15.00.10/y_hat/T-5.npy  \n",
            "  inflating: data/2018-05-19_15.00.10/y_hat/T-8.npy  \n",
            "  inflating: data/2018-05-19_15.00.10/y_hat/T-9.npy  \n",
            "   creating: data/test/\n",
            "  inflating: data/test/A-1.npy       \n",
            "  inflating: data/test/A-2.npy       \n",
            "  inflating: data/test/A-3.npy       \n",
            "  inflating: data/test/A-4.npy       \n",
            "  inflating: data/test/A-5.npy       \n",
            "  inflating: data/test/A-6.npy       \n",
            "  inflating: data/test/A-7.npy       \n",
            "  inflating: data/test/A-8.npy       \n",
            "  inflating: data/test/A-9.npy       \n",
            "  inflating: data/test/B-1.npy       \n",
            "  inflating: data/test/C-1.npy       \n",
            "  inflating: data/test/C-2.npy       \n",
            "  inflating: data/test/D-1.npy       \n",
            "  inflating: data/test/D-11.npy      \n",
            "  inflating: data/test/D-12.npy      \n",
            "  inflating: data/test/D-13.npy      \n",
            "  inflating: data/test/D-14.npy      \n",
            "  inflating: data/test/D-15.npy      \n",
            "  inflating: data/test/D-16.npy      \n",
            "  inflating: data/test/D-2.npy       \n",
            "  inflating: data/test/D-3.npy       \n",
            "  inflating: data/test/D-4.npy       \n",
            "  inflating: data/test/D-5.npy       \n",
            "  inflating: data/test/D-6.npy       \n",
            "  inflating: data/test/D-7.npy       \n",
            "  inflating: data/test/D-8.npy       \n",
            "  inflating: data/test/D-9.npy       \n",
            "  inflating: data/test/E-1.npy       \n",
            "  inflating: data/test/E-10.npy      \n",
            "  inflating: data/test/E-11.npy      \n",
            "  inflating: data/test/E-12.npy      \n",
            "  inflating: data/test/E-13.npy      \n",
            "  inflating: data/test/E-2.npy       \n",
            "  inflating: data/test/E-3.npy       \n",
            "  inflating: data/test/E-4.npy       \n",
            "  inflating: data/test/E-5.npy       \n",
            "  inflating: data/test/E-6.npy       \n",
            "  inflating: data/test/E-7.npy       \n",
            "  inflating: data/test/E-8.npy       \n",
            "  inflating: data/test/E-9.npy       \n",
            "  inflating: data/test/F-1.npy       \n",
            "  inflating: data/test/F-2.npy       \n",
            "  inflating: data/test/F-3.npy       \n",
            "  inflating: data/test/F-4.npy       \n",
            "  inflating: data/test/F-5.npy       \n",
            "  inflating: data/test/F-7.npy       \n",
            "  inflating: data/test/F-8.npy       \n",
            "  inflating: data/test/G-1.npy       \n",
            "  inflating: data/test/G-2.npy       \n",
            "  inflating: data/test/G-3.npy       \n",
            "  inflating: data/test/G-4.npy       \n",
            "  inflating: data/test/G-6.npy       \n",
            "  inflating: data/test/G-7.npy       \n",
            "  inflating: data/test/M-1.npy       \n",
            "  inflating: data/test/M-2.npy       \n",
            "  inflating: data/test/M-3.npy       \n",
            "  inflating: data/test/M-4.npy       \n",
            "  inflating: data/test/M-5.npy       \n",
            "  inflating: data/test/M-6.npy       \n",
            "  inflating: data/test/M-7.npy       \n",
            "  inflating: data/test/P-1.npy       \n",
            "  inflating: data/test/P-10.npy      \n",
            "  inflating: data/test/P-11.npy      \n",
            "  inflating: data/test/P-14.npy      \n",
            "  inflating: data/test/P-15.npy      \n",
            "  inflating: data/test/P-2.npy       \n",
            "  inflating: data/test/P-3.npy       \n",
            "  inflating: data/test/P-4.npy       \n",
            "  inflating: data/test/P-7.npy       \n",
            "  inflating: data/test/R-1.npy       \n",
            "  inflating: data/test/S-1.npy       \n",
            "  inflating: data/test/S-2.npy       \n",
            "  inflating: data/test/T-1.npy       \n",
            "  inflating: data/test/T-10.npy      \n",
            "  inflating: data/test/T-12.npy      \n",
            "  inflating: data/test/T-13.npy      \n",
            "  inflating: data/test/T-2.npy       \n",
            "  inflating: data/test/T-3.npy       \n",
            "  inflating: data/test/T-4.npy       \n",
            "  inflating: data/test/T-5.npy       \n",
            "  inflating: data/test/T-8.npy       \n",
            "  inflating: data/test/T-9.npy       \n",
            "   creating: data/train/\n",
            "  inflating: data/train/A-1.npy      \n",
            "  inflating: data/train/A-2.npy      \n",
            "  inflating: data/train/A-3.npy      \n",
            "  inflating: data/train/A-4.npy      \n",
            "  inflating: data/train/A-5.npy      \n",
            "  inflating: data/train/A-6.npy      \n",
            "  inflating: data/train/A-7.npy      \n",
            "  inflating: data/train/A-8.npy      \n",
            "  inflating: data/train/A-9.npy      \n",
            "  inflating: data/train/B-1.npy      \n",
            "  inflating: data/train/C-1.npy      \n",
            "  inflating: data/train/C-2.npy      \n",
            "  inflating: data/train/D-1.npy      \n",
            "  inflating: data/train/D-11.npy     \n",
            "  inflating: data/train/D-12.npy     \n",
            "  inflating: data/train/D-13.npy     \n",
            "  inflating: data/train/D-14.npy     \n",
            "  inflating: data/train/D-15.npy     \n",
            "  inflating: data/train/D-16.npy     \n",
            "  inflating: data/train/D-2.npy      \n",
            "  inflating: data/train/D-3.npy      \n",
            "  inflating: data/train/D-4.npy      \n",
            "  inflating: data/train/D-5.npy      \n",
            "  inflating: data/train/D-6.npy      \n",
            "  inflating: data/train/D-7.npy      \n",
            "  inflating: data/train/D-8.npy      \n",
            "  inflating: data/train/D-9.npy      \n",
            "  inflating: data/train/E-1.npy      \n",
            "  inflating: data/train/E-10.npy     \n",
            "  inflating: data/train/E-11.npy     \n",
            "  inflating: data/train/E-12.npy     \n",
            "  inflating: data/train/E-13.npy     \n",
            "  inflating: data/train/E-2.npy      \n",
            "  inflating: data/train/E-3.npy      \n",
            "  inflating: data/train/E-4.npy      \n",
            "  inflating: data/train/E-5.npy      \n",
            "  inflating: data/train/E-6.npy      \n",
            "  inflating: data/train/E-7.npy      \n",
            "  inflating: data/train/E-8.npy      \n",
            "  inflating: data/train/E-9.npy      \n",
            "  inflating: data/train/F-1.npy      \n",
            "  inflating: data/train/F-2.npy      \n",
            "  inflating: data/train/F-3.npy      \n",
            "  inflating: data/train/F-4.npy      \n",
            "  inflating: data/train/F-5.npy      \n",
            "  inflating: data/train/F-7.npy      \n",
            "  inflating: data/train/F-8.npy      \n",
            "  inflating: data/train/G-1.npy      \n",
            "  inflating: data/train/G-2.npy      \n",
            "  inflating: data/train/G-3.npy      \n",
            "  inflating: data/train/G-4.npy      \n",
            "  inflating: data/train/G-6.npy      \n",
            "  inflating: data/train/G-7.npy      \n",
            "  inflating: data/train/M-1.npy      \n",
            "  inflating: data/train/M-2.npy      \n",
            "  inflating: data/train/M-3.npy      \n",
            "  inflating: data/train/M-4.npy      \n",
            "  inflating: data/train/M-5.npy      \n",
            "  inflating: data/train/M-6.npy      \n",
            "  inflating: data/train/M-7.npy      \n",
            "  inflating: data/train/P-1.npy      \n",
            "  inflating: data/train/P-10.npy     \n",
            "  inflating: data/train/P-11.npy     \n",
            "  inflating: data/train/P-14.npy     \n",
            "  inflating: data/train/P-15.npy     \n",
            "  inflating: data/train/P-2.npy      \n",
            "  inflating: data/train/P-3.npy      \n",
            "  inflating: data/train/P-4.npy      \n",
            "  inflating: data/train/P-7.npy      \n",
            "  inflating: data/train/R-1.npy      \n",
            "  inflating: data/train/S-1.npy      \n",
            "  inflating: data/train/S-2.npy      \n",
            "  inflating: data/train/T-1.npy      \n",
            "  inflating: data/train/T-10.npy     \n",
            "  inflating: data/train/T-12.npy     \n",
            "  inflating: data/train/T-13.npy     \n",
            "  inflating: data/train/T-2.npy      \n",
            "  inflating: data/train/T-3.npy      \n",
            "  inflating: data/train/T-4.npy      \n",
            "  inflating: data/train/T-5.npy      \n",
            "  inflating: data/train/T-8.npy      \n",
            "  inflating: data/train/T-9.npy      \n",
            "--2023-01-09 08:17:50--  https://raw.githubusercontent.com/khundman/telemanom/master/labeled_anomalies.csv\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 3956 (3.9K) [text/plain]\n",
            "Saving to: ‘labeled_anomalies.csv’\n",
            "\n",
            "labeled_anomalies.c 100%[===================>]   3.86K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-01-09 08:17:50 (41.0 MB/s) - ‘labeled_anomalies.csv’ saved [3956/3956]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## NAB"
      ],
      "metadata": {
        "id": "7O3epkm_izR1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = 'NAB'\n",
        "!git clone https://github.com/numenta/NAB.git\n",
        "shutil.move('/content/NAB/data', '/content/original_data/NAB')\n",
        "shutil.move('/content/NAB/labels', '/content/original_data/NAB')\n",
        "shutil.rmtree('/content/NAB')\n",
        "prepare_data(dataset)"
      ],
      "metadata": {
        "id": "80y80Wt4evGb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4be02e07-e9eb-4c51-b6b3-9bb9bcaaee51"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'NAB'...\n",
            "remote: Enumerating objects: 7029, done.\u001b[K\n",
            "remote: Counting objects: 100% (114/114), done.\u001b[K\n",
            "remote: Compressing objects: 100% (78/78), done.\u001b[K\n",
            "remote: Total 7029 (delta 48), reused 58 (delta 35), pack-reused 6915\u001b[K\n",
            "Receiving objects: 100% (7029/7029), 86.74 MiB | 23.84 MiB/s, done.\n",
            "Resolving deltas: 100% (4922/4922), done.\n",
            "Checking out files: 100% (1186/1186), done.\n",
            "ec2_request_latency_system_failure.csv shape (4032, 2)\n",
            "nyc_taxi.csv shape (10320, 2)\n",
            "cpu_utilization_asg_misconfiguration.csv shape (18050, 2)\n",
            "rogue_agent_key_updown.csv shape (5315, 2)\n",
            "rogue_agent_key_hold.csv shape (1882, 2)\n",
            "ambient_temperature_system_failure.csv shape (7267, 2)\n",
            "machine_temperature_system_failure.csv shape (22695, 2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## SMD"
      ],
      "metadata": {
        "id": "It7i9TzI-2Ff"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = 'SMD'\n",
        "!git clone https://github.com/NetManAIOps/OmniAnomaly.git\n",
        "shutil.move('/content/OmniAnomaly/ServerMachineDataset', '/content/original_data/SMD')\n",
        "shutil.rmtree('/content/OmniAnomaly')\n",
        "prepare_data(dataset)"
      ],
      "metadata": {
        "id": "zHb0Gxntfhhk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7cc3cf93-5817-4c1b-edf8-70ea516e084d"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'OmniAnomaly'...\n",
            "remote: Enumerating objects: 204, done.\u001b[K\n",
            "remote: Counting objects: 100% (6/6), done.\u001b[K\n",
            "remote: Compressing objects: 100% (6/6), done.\u001b[K\n",
            "remote: Total 204 (delta 2), reused 0 (delta 0), pack-reused 198\u001b[K\n",
            "Receiving objects: 100% (204/204), 107.11 MiB | 13.98 MiB/s, done.\n",
            "Resolving deltas: 100% (80/80), done.\n",
            "Checking out files: 100% (132/132), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##WADI\n"
      ],
      "metadata": {
        "id": "pQG5UkQ5ktsk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = 'WADI'\n",
        "# you can download https://itrust.sutd.edu.sg/itrust-labs_datasets/.\n",
        "# Download the data and upload it to Google Drive. You can put the path there here.\n",
        "dataset_folder = '/content/drive/MyDrive/WADI/WADI.A2_19 Nov 2019' ## your path!\n",
        "prepare_data(dataset, dataset_folder=dataset_folder)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ttR4aklkktYt",
        "outputId": "fecf89e9-f231-47ec-c674-0482569cceac"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train shape:(784571, 122), test shape :(172801, 122), label shape:(172801, 1)\n",
            "Exclude 10/2/17 data\n",
            "Exclude 9/29/2017 data\n",
            "Exclude 10/7/17 data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "oEw4xfgfm6j3"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}