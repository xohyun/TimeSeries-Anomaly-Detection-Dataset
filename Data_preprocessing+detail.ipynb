{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1f1jzgzneYsfbpAJFz56mvlR0nXHpRXnQ",
      "authorship_tag": "ABX9TyM4RuVOT6f7x/2xnnY64mdP",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/xohyun/TimeSeries-Anomaly-Detection-Dataset/blob/master/Data_preprocessing%2Bdetail.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Implementation of preprocessing.\n",
        "References\n",
        "    - https://github.com/imperial-qore/TranAD/blob/main/preprocess.py\n",
        "\"\"\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "esJrC-fT-PaR",
        "outputId": "b24251f9-9ec5-4ef7-9757-9d07fe1d99be"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nImplementation of preprocessing.\\nReferences\\n    - https://github.com/imperial-qore/TranAD/blob/main/preprocess.py\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Drive mount"
      ],
      "metadata": {
        "id": "sqk70vS38nR1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GPAGhPm-b0Ca",
        "outputId": "beae6d6b-ae54-446a-80c0-1e9f268dc31c"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Import"
      ],
      "metadata": {
        "id": "64ZIF_Wz8sBu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import shutil\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import matplotlib.pyplot as plt\n",
        "import json\n",
        "import datetime"
      ],
      "metadata": {
        "id": "rBYpbJKuch4N"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "zvZDD8n3btYj"
      },
      "outputs": [],
      "source": [
        "def create_folder(directory):\n",
        "    '''\n",
        "    Create folder\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    directory : path and directory that you want to create\n",
        "    \n",
        "    Returns\n",
        "    -------\n",
        "    None\n",
        "    '''\n",
        "    try:\n",
        "        if not os.path.exists(directory):\n",
        "            os.makedirs(directory)\n",
        "    except OSError:\n",
        "        print ('Error: Creating directory ' +  directory)\n",
        "\n",
        "def prepare_data(dataset, *args, **kwargs):\n",
        "    '''\n",
        "    Preprocessing function + save dataset as numpy file\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    dataset : dataset that you want to preprocess (WADI, MSL, SMAP, NAB, SMD)\n",
        "    \n",
        "    Returns\n",
        "    -------\n",
        "    None\n",
        "    '''\n",
        "    if dataset == 'WADI':\n",
        "        dataset_folder = kwargs['dataset_folder']\n",
        "        output_folder = '/content/data/WADI'\n",
        "        create_folder(output_folder)\n",
        "\n",
        "        train = pd.read_csv(os.path.join(dataset_folder, 'WADI_14days_new.csv'))\n",
        "        test = pd.read_csv(os.path.join(dataset_folder, 'WADI_attackdataLABLE.csv'), header=0, low_memory=False, index_col=0)\n",
        "        test = test.rename(columns=test.iloc[0]).iloc[1:].reset_index() # replace first row with column name\n",
        "\n",
        "        label = test[['Attack LABLE (1:No Attack, -1:Attack)']]\n",
        "        test = test.drop(['Attack LABLE (1:No Attack, -1:Attack)'], axis='columns')\n",
        "\n",
        "        row = test[test.isna().all(axis=1)].index.tolist() # find the index of the row in which all columns are NA\n",
        "        label = label.drop(row, axis=0).replace(['-1', '1'], [1, 0]) # 1 : anomaly\n",
        "\n",
        "        train = train.dropna(how='all', axis=0) #inplace=True\n",
        "        test = test.dropna(how='all', axis=0) #inplace=True\n",
        "\n",
        "        col = train.columns[train.isna().any()] # find the index of the columns in which rows are NA\n",
        "        train = train.dropna(axis=1)\n",
        "        test = test.drop(col, axis=1)\n",
        "\n",
        "        print(f\"train shape:{train.shape}, test shape :{test.shape}, label shape:{label.shape}\")\n",
        "\n",
        "        #---# To save train data #---#\n",
        "        for i in set(train['Date']):\n",
        "            if len(train[train['Date'] == i]) % 60 != 0:\n",
        "                print(f\"Exclude {i} data\")\n",
        "                continue\n",
        "            df_train = train[train['Date'] == i]\n",
        "            \n",
        "            start = pd.to_datetime(df_train.iloc[0]['Date'] + ' ' + df_train.iloc[0]['Time'])    \n",
        "            date_range = pd.date_range(start = start, periods=len(df_train), freq='s')\n",
        "            df_train = df_train.drop(['Date', 'Time'], axis = 1)\n",
        "            df_train.insert(1, 'Time', date_range)\n",
        "            \n",
        "            #---# MinMaxScaler #---#\n",
        "            values = df_train.iloc[:, 2:] # Exclude the time and index columns\n",
        "            scaler = MinMaxScaler()\n",
        "            values = scaler.fit_transform(values)\n",
        "            df_train.iloc[:, 2:] = values\n",
        "\n",
        "            date = i.replace('/', '_')\n",
        "            np.save(os.path.join(output_folder, f\"{date}_train.npy\"), df_train.to_numpy())\n",
        "\n",
        "        #---# To save test data, label data #---#\n",
        "        for i in set(test['Date ']):\n",
        "            df_test = test[test['Date '] == i]\n",
        "            df_label = label[test['Date '] == i]\n",
        "            \n",
        "            start = pd.to_datetime(df_test.iloc[0]['Date '] + ' ' + df_test.iloc[0]['Time'])\n",
        "            # end = start + datetime.timedelta(hours=int(len(df_test) / (60*60)))\n",
        "            # pd.date_range(start = start, end = end, freq='1s')\n",
        "            \n",
        "            date_range = pd.date_range(start = start, periods=len(df_test), freq='s')\n",
        "            df_test = df_test.drop(['Date ', 'Time'], axis = 1)\n",
        "            df_test.insert(1, 'Time', date_range)\n",
        "\n",
        "            #---# MinMaxScaler #---#\n",
        "            values = df_test.iloc[:, 2:] # Exclude the time and index columns\n",
        "            scaler = MinMaxScaler()\n",
        "            values = scaler.fit_transform(values)\n",
        "            df_test.iloc[:, 2:] = values\n",
        "\n",
        "            date = i.replace('/', '_')\n",
        "            np.save(os.path.join(output_folder, f\"{date}_test.npy\"), df_test.to_numpy())\n",
        "            np.save(os.path.join(output_folder, f\"{date}_labels.npy\"), df_label.to_numpy())\n",
        "\n",
        "    elif dataset == 'MSL' or dataset == 'SMAP':\n",
        "        # choose_data = ['A-4', 'C-2', 'T-1']\n",
        "\n",
        "        dataset_folder = '/content/original_data/SMAP_MSL'\n",
        "        output_folder = os.path.join('/content/data', dataset)\n",
        "        create_folder(output_folder)\n",
        "\n",
        "        file_ = os.path.join(dataset_folder, 'labeled_anomalies.csv')\n",
        "        values = pd.read_csv(file_)\n",
        "\n",
        "        values = values[values['spacecraft'] == dataset]\n",
        "        filenames = values['chan_id'].values.tolist()    \n",
        "\n",
        "        for fn in filenames:\n",
        "            # if fn not in choose_data:\n",
        "            #     continue\n",
        "            train = np.load(f'{dataset_folder}/train/{fn}.npy')\n",
        "            test = np.load(f'{dataset_folder}/test/{fn}.npy')\n",
        "\n",
        "            #---# MinMaxScaler #---#\n",
        "            scaler = MinMaxScaler()\n",
        "            train = scaler.fit_transform(train)\n",
        "            test = scaler.transform(test)\n",
        "            \n",
        "            #---# save train.npy and test.npy #---#\n",
        "            np.save(f'{output_folder}/{fn}_train.npy', train)\n",
        "            np.save(f'{output_folder}/{fn}_test.npy', test)\n",
        "\n",
        "            #---# save labels.npy #---#\n",
        "            labels = np.zeros(test.shape)\n",
        "            indices = values[values['chan_id'] == fn]['anomaly_sequences'].values[0]\n",
        "            indices = indices.replace(']', '').replace('[', '').split(', ')\n",
        "            indices = [int(i) for i in indices]\n",
        "            for i in range(0, len(indices), 2):\n",
        "                labels[indices[i]:indices[i+1], :] = 1\n",
        "            np.save(f'{output_folder}/{fn}_labels.npy', labels)\n",
        "  \n",
        "    elif dataset == 'NAB':\n",
        "        dataset_folder = 'original_data/NAB/realKnownCause'\n",
        "        label_folder = 'original_data/NAB/labels'\n",
        "        output_folder = 'data/NAB'\n",
        "        create_folder(output_folder)\n",
        "\n",
        "        file_list = os.listdir(dataset_folder)\n",
        "\n",
        "        with open(label_folder + '/combined_windows.json') as f:\n",
        "            labeldict = json.load(f)\n",
        "\n",
        "        for filename in file_list:\n",
        "            if not filename.endswith('.csv'): continue\n",
        "            df = pd.read_csv(dataset_folder+'/'+filename)\n",
        "        \n",
        "            print(f\"{filename} shape {df.shape}\")\n",
        "            values = df.values[:,1]\n",
        "            \n",
        "            #---# MinMaxScaler #---#\n",
        "            scaler = MinMaxScaler()\n",
        "            values = scaler.fit_transform(values.reshape(-1,1))\n",
        "\n",
        "            #---# Label #---#\n",
        "            labels = np.zeros_like(values, dtype=np.float64)\n",
        "            for timestamp in labeldict['realKnownCause/'+filename]:\n",
        "                tstamp = timestamp[0].replace('.000000', '')\n",
        "                start_index = np.where(((df['timestamp'] == tstamp).values + 0) == 1)[0][0]\n",
        "                tstamp = timestamp[1].replace('.000000', '')\n",
        "                end_index = np.where(((df['timestamp'] == tstamp).values + 0) == 1)[0][0]\n",
        "                labels[start_index : end_index] = 1\n",
        "\n",
        "            #---# Split train npy and test npy #---#\n",
        "            train, test = values, values\n",
        "            train, test, labels = train.reshape(-1, 1), test.reshape(-1, 1), labels.reshape(-1, 1)\n",
        "            \n",
        "            #---# Save file #---#\n",
        "            fn = filename.replace('.csv', '')\n",
        "            for file in ['train', 'test', 'labels']:\n",
        "                np.save(os.path.join(output_folder, f'{fn}_{file}.npy'), eval(file))\n",
        "\n",
        "    elif dataset == 'SMD':\n",
        "        dataset_folder = '/content/original_data/SMD'\n",
        "        output_folder = '/content/data/SMD'\n",
        "        create_folder(output_folder)\n",
        "\n",
        "        file_list = os.listdir(os.path.join(dataset_folder, \"train\"))\n",
        "        for filename in file_list:\n",
        "            if filename.endswith('.txt'):\n",
        "                #---# train #---#\n",
        "                values_train = np.genfromtxt(os.path.join(dataset_folder, 'train', filename), delimiter=',')\n",
        "                scaler = MinMaxScaler()\n",
        "                values_train_scale = scaler.fit_transform(values_train)\n",
        "                np.save(os.path.join(output_folder, f\"train_{filename}.npy\"), values_train_scale)\n",
        "                \n",
        "                #---# test #---#\n",
        "                values_test = np.genfromtxt(os.path.join(dataset_folder, 'test', filename), delimiter=',')\n",
        "                scaler = MinMaxScaler()\n",
        "                values_test_scale = scaler.fit_transform(values_train)\n",
        "                np.save(os.path.join(output_folder, f\"test_{filename}.npy\"), values_test_scale)\n",
        "\n",
        "                #---# label #---#\n",
        "                values_label = np.genfromtxt(os.path.join(dataset_folder, 'test_label', filename), delimiter=',')\n",
        "                np.save(os.path.join(output_folder, f\"label_{filename}.npy\"), values_label)\n",
        "\n",
        "                #---# interpretation_label #---#\n",
        "                temp = np.zeros(values_test.shape)\n",
        "                with open(os.path.join(dataset_folder, 'interpretation_label', filename), \"r\") as f:\n",
        "                    ls = f.readlines()\n",
        "                    for line in ls:\n",
        "                        pos, value = line.split(':')[0], line.split(':')[1].split(',')\n",
        "                        start, end, inx = int(pos.split('-')[0]), int(pos.split('-')[1]), [int(i)-1 for i in value]\n",
        "                        temp[start-1:end-1, inx] = 1\n",
        "                        np.save(os.path.join(output_folder, f\"label_{filename}_interpret.npy\"), temp)\n",
        "\n",
        "    else:\n",
        "        print(\"Check the dataset!!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## SMAP & MSL"
      ],
      "metadata": {
        "id": "BSCVyTzciwHF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://s3-us-west-2.amazonaws.com/telemanom/data.zip && unzip data.zip && rm data.zip\n",
        "!cd data && wget https://raw.githubusercontent.com/khundman/telemanom/master/labeled_anomalies.csv\n",
        "!mkdir original_data\n",
        "os.rename('/content/data', '/content/SMAP_MSL')\n",
        "shutil.move('/content/SMAP_MSL', '/content/original_data/SMAP_MSL')\n",
        "\n",
        "dataset = 'SMAP'\n",
        "prepare_data(dataset)\n",
        "\n",
        "dataset = 'MSL'\n",
        "prepare_data(dataset)"
      ],
      "metadata": {
        "id": "ZkVtxnvYgQLg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ce874822-9f96-41a3-fafd-fdf2afbc7cbe"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-01-09 08:10:50--  https://s3-us-west-2.amazonaws.com/telemanom/data.zip\n",
            "Resolving s3-us-west-2.amazonaws.com (s3-us-west-2.amazonaws.com)... 52.92.149.64, 52.218.243.24, 52.92.195.32, ...\n",
            "Connecting to s3-us-west-2.amazonaws.com (s3-us-west-2.amazonaws.com)|52.92.149.64|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 85899803 (82M) [application/zip]\n",
            "Saving to: ‘data.zip’\n",
            "\n",
            "data.zip            100%[===================>]  81.92M  27.3MB/s    in 3.0s    \n",
            "\n",
            "2023-01-09 08:10:54 (27.3 MB/s) - ‘data.zip’ saved [85899803/85899803]\n",
            "\n",
            "Archive:  data.zip\n",
            "   creating: data/2018-05-19_15.00.10/\n",
            "   creating: data/2018-05-19_15.00.10/models/\n",
            "  inflating: data/2018-05-19_15.00.10/models/A-1.h5  \n",
            "  inflating: data/2018-05-19_15.00.10/models/A-2.h5  \n",
            "  inflating: data/2018-05-19_15.00.10/models/A-3.h5  \n",
            "  inflating: data/2018-05-19_15.00.10/models/A-4.h5  \n",
            "  inflating: data/2018-05-19_15.00.10/models/A-5.h5  \n",
            "  inflating: data/2018-05-19_15.00.10/models/A-6.h5  \n",
            "  inflating: data/2018-05-19_15.00.10/models/A-7.h5  \n",
            "  inflating: data/2018-05-19_15.00.10/models/A-8.h5  \n",
            "  inflating: data/2018-05-19_15.00.10/models/A-9.h5  \n",
            "  inflating: data/2018-05-19_15.00.10/models/B-1.h5  \n",
            "  inflating: data/2018-05-19_15.00.10/models/C-1.h5  \n",
            "  inflating: data/2018-05-19_15.00.10/models/C-2.h5  \n",
            "  inflating: data/2018-05-19_15.00.10/models/D-1.h5  \n",
            "  inflating: data/2018-05-19_15.00.10/models/D-11.h5  \n",
            "  inflating: data/2018-05-19_15.00.10/models/D-12.h5  \n",
            "  inflating: data/2018-05-19_15.00.10/models/D-13.h5  \n",
            "  inflating: data/2018-05-19_15.00.10/models/D-14.h5  \n",
            "  inflating: data/2018-05-19_15.00.10/models/D-15.h5  \n",
            "  inflating: data/2018-05-19_15.00.10/models/D-16.h5  \n",
            "  inflating: data/2018-05-19_15.00.10/models/D-2.h5  \n",
            "  inflating: data/2018-05-19_15.00.10/models/D-3.h5  \n",
            "  inflating: data/2018-05-19_15.00.10/models/D-4.h5  \n",
            "  inflating: data/2018-05-19_15.00.10/models/D-5.h5  \n",
            "  inflating: data/2018-05-19_15.00.10/models/D-6.h5  \n",
            "  inflating: data/2018-05-19_15.00.10/models/D-7.h5  \n",
            "  inflating: data/2018-05-19_15.00.10/models/D-8.h5  \n",
            "  inflating: data/2018-05-19_15.00.10/models/D-9.h5  \n",
            "  inflating: data/2018-05-19_15.00.10/models/E-1.h5  \n",
            "  inflating: data/2018-05-19_15.00.10/models/E-10.h5  \n",
            "  inflating: data/2018-05-19_15.00.10/models/E-11.h5  \n",
            "  inflating: data/2018-05-19_15.00.10/models/E-12.h5  \n",
            "  inflating: data/2018-05-19_15.00.10/models/E-13.h5  \n",
            "  inflating: data/2018-05-19_15.00.10/models/E-2.h5  \n",
            "  inflating: data/2018-05-19_15.00.10/models/E-3.h5  \n",
            "  inflating: data/2018-05-19_15.00.10/models/E-4.h5  \n",
            "  inflating: data/2018-05-19_15.00.10/models/E-5.h5  \n",
            "  inflating: data/2018-05-19_15.00.10/models/E-6.h5  \n",
            "  inflating: data/2018-05-19_15.00.10/models/E-7.h5  \n",
            "  inflating: data/2018-05-19_15.00.10/models/E-8.h5  \n",
            "  inflating: data/2018-05-19_15.00.10/models/E-9.h5  \n",
            "  inflating: data/2018-05-19_15.00.10/models/F-1.h5  \n",
            "  inflating: data/2018-05-19_15.00.10/models/F-2.h5  \n",
            "  inflating: data/2018-05-19_15.00.10/models/F-3.h5  \n",
            "  inflating: data/2018-05-19_15.00.10/models/F-4.h5  \n",
            "  inflating: data/2018-05-19_15.00.10/models/F-5.h5  \n",
            "  inflating: data/2018-05-19_15.00.10/models/F-7.h5  \n",
            "  inflating: data/2018-05-19_15.00.10/models/F-8.h5  \n",
            "  inflating: data/2018-05-19_15.00.10/models/G-1.h5  \n",
            "  inflating: data/2018-05-19_15.00.10/models/G-2.h5  \n",
            "  inflating: data/2018-05-19_15.00.10/models/G-3.h5  \n",
            "  inflating: data/2018-05-19_15.00.10/models/G-4.h5  \n",
            "  inflating: data/2018-05-19_15.00.10/models/G-6.h5  \n",
            "  inflating: data/2018-05-19_15.00.10/models/G-7.h5  \n",
            "  inflating: data/2018-05-19_15.00.10/models/M-1.h5  \n",
            "  inflating: data/2018-05-19_15.00.10/models/M-2.h5  \n",
            "  inflating: data/2018-05-19_15.00.10/models/M-3.h5  \n",
            "  inflating: data/2018-05-19_15.00.10/models/M-4.h5  \n",
            "  inflating: data/2018-05-19_15.00.10/models/M-5.h5  \n",
            "  inflating: data/2018-05-19_15.00.10/models/M-6.h5  \n",
            "  inflating: data/2018-05-19_15.00.10/models/M-7.h5  \n",
            "  inflating: data/2018-05-19_15.00.10/models/P-1.h5  \n",
            "  inflating: data/2018-05-19_15.00.10/models/P-10.h5  \n",
            "  inflating: data/2018-05-19_15.00.10/models/P-11.h5  \n",
            "  inflating: data/2018-05-19_15.00.10/models/P-14.h5  \n",
            "  inflating: data/2018-05-19_15.00.10/models/P-15.h5  \n",
            "  inflating: data/2018-05-19_15.00.10/models/P-2.h5  \n",
            "  inflating: data/2018-05-19_15.00.10/models/P-3.h5  \n",
            "  inflating: data/2018-05-19_15.00.10/models/P-4.h5  \n",
            "  inflating: data/2018-05-19_15.00.10/models/P-7.h5  \n",
            "  inflating: data/2018-05-19_15.00.10/models/R-1.h5  \n",
            "  inflating: data/2018-05-19_15.00.10/models/S-1.h5  \n",
            "  inflating: data/2018-05-19_15.00.10/models/S-2.h5  \n",
            "  inflating: data/2018-05-19_15.00.10/models/T-1.h5  \n",
            "  inflating: data/2018-05-19_15.00.10/models/T-10.h5  \n",
            "  inflating: data/2018-05-19_15.00.10/models/T-12.h5  \n",
            "  inflating: data/2018-05-19_15.00.10/models/T-13.h5  \n",
            "  inflating: data/2018-05-19_15.00.10/models/T-2.h5  \n",
            "  inflating: data/2018-05-19_15.00.10/models/T-3.h5  \n",
            "  inflating: data/2018-05-19_15.00.10/models/T-4.h5  \n",
            "  inflating: data/2018-05-19_15.00.10/models/T-5.h5  \n",
            "  inflating: data/2018-05-19_15.00.10/models/T-8.h5  \n",
            "  inflating: data/2018-05-19_15.00.10/models/T-9.h5  \n",
            "  inflating: data/2018-05-19_15.00.10/params.log  \n",
            "   creating: data/2018-05-19_15.00.10/smoothed_errors/\n",
            "  inflating: data/2018-05-19_15.00.10/smoothed_errors/A-1.npy  \n",
            "  inflating: data/2018-05-19_15.00.10/smoothed_errors/A-2.npy  \n",
            "  inflating: data/2018-05-19_15.00.10/smoothed_errors/A-3.npy  \n",
            "  inflating: data/2018-05-19_15.00.10/smoothed_errors/A-4.npy  \n",
            "  inflating: data/2018-05-19_15.00.10/smoothed_errors/A-5.npy  \n",
            "  inflating: data/2018-05-19_15.00.10/smoothed_errors/A-6.npy  \n",
            "  inflating: data/2018-05-19_15.00.10/smoothed_errors/A-7.npy  \n",
            "  inflating: data/2018-05-19_15.00.10/smoothed_errors/A-8.npy  \n",
            "  inflating: data/2018-05-19_15.00.10/smoothed_errors/A-9.npy  \n",
            "  inflating: data/2018-05-19_15.00.10/smoothed_errors/B-1.npy  \n",
            "  inflating: data/2018-05-19_15.00.10/smoothed_errors/C-1.npy  \n",
            "  inflating: data/2018-05-19_15.00.10/smoothed_errors/C-2.npy  \n",
            "  inflating: data/2018-05-19_15.00.10/smoothed_errors/D-1.npy  \n",
            "  inflating: data/2018-05-19_15.00.10/smoothed_errors/D-11.npy  \n",
            "  inflating: data/2018-05-19_15.00.10/smoothed_errors/D-12.npy  \n",
            "  inflating: data/2018-05-19_15.00.10/smoothed_errors/D-13.npy  \n",
            "  inflating: data/2018-05-19_15.00.10/smoothed_errors/D-14.npy  \n",
            "  inflating: data/2018-05-19_15.00.10/smoothed_errors/D-15.npy  \n",
            "  inflating: data/2018-05-19_15.00.10/smoothed_errors/D-16.npy  \n",
            "  inflating: data/2018-05-19_15.00.10/smoothed_errors/D-2.npy  \n",
            "  inflating: data/2018-05-19_15.00.10/smoothed_errors/D-3.npy  \n",
            "  inflating: data/2018-05-19_15.00.10/smoothed_errors/D-4.npy  \n",
            "  inflating: data/2018-05-19_15.00.10/smoothed_errors/D-5.npy  \n",
            "  inflating: data/2018-05-19_15.00.10/smoothed_errors/D-6.npy  \n",
            "  inflating: data/2018-05-19_15.00.10/smoothed_errors/D-7.npy  \n",
            "  inflating: data/2018-05-19_15.00.10/smoothed_errors/D-8.npy  \n",
            "  inflating: data/2018-05-19_15.00.10/smoothed_errors/D-9.npy  \n",
            "  inflating: data/2018-05-19_15.00.10/smoothed_errors/E-1.npy  \n",
            "  inflating: data/2018-05-19_15.00.10/smoothed_errors/E-10.npy  \n",
            "  inflating: data/2018-05-19_15.00.10/smoothed_errors/E-11.npy  \n",
            "  inflating: data/2018-05-19_15.00.10/smoothed_errors/E-12.npy  \n",
            "  inflating: data/2018-05-19_15.00.10/smoothed_errors/E-13.npy  \n",
            "  inflating: data/2018-05-19_15.00.10/smoothed_errors/E-2.npy  \n",
            "  inflating: data/2018-05-19_15.00.10/smoothed_errors/E-3.npy  \n",
            "  inflating: data/2018-05-19_15.00.10/smoothed_errors/E-4.npy  \n",
            "  inflating: data/2018-05-19_15.00.10/smoothed_errors/E-5.npy  \n",
            "  inflating: data/2018-05-19_15.00.10/smoothed_errors/E-6.npy  \n",
            "  inflating: data/2018-05-19_15.00.10/smoothed_errors/E-7.npy  \n",
            "  inflating: data/2018-05-19_15.00.10/smoothed_errors/E-8.npy  \n",
            "  inflating: data/2018-05-19_15.00.10/smoothed_errors/E-9.npy  \n",
            "  inflating: data/2018-05-19_15.00.10/smoothed_errors/F-1.npy  \n",
            "  inflating: data/2018-05-19_15.00.10/smoothed_errors/F-2.npy  \n",
            "  inflating: data/2018-05-19_15.00.10/smoothed_errors/F-3.npy  \n",
            "  inflating: data/2018-05-19_15.00.10/smoothed_errors/F-4.npy  \n",
            "  inflating: data/2018-05-19_15.00.10/smoothed_errors/F-5.npy  \n",
            "  inflating: data/2018-05-19_15.00.10/smoothed_errors/F-7.npy  \n",
            "  inflating: data/2018-05-19_15.00.10/smoothed_errors/F-8.npy  \n",
            "  inflating: data/2018-05-19_15.00.10/smoothed_errors/G-1.npy  \n",
            "  inflating: data/2018-05-19_15.00.10/smoothed_errors/G-2.npy  \n",
            "  inflating: data/2018-05-19_15.00.10/smoothed_errors/G-3.npy  \n",
            "  inflating: data/2018-05-19_15.00.10/smoothed_errors/G-4.npy  \n",
            "  inflating: data/2018-05-19_15.00.10/smoothed_errors/G-6.npy  \n",
            "  inflating: data/2018-05-19_15.00.10/smoothed_errors/G-7.npy  \n",
            "  inflating: data/2018-05-19_15.00.10/smoothed_errors/M-1.npy  \n",
            "  inflating: data/2018-05-19_15.00.10/smoothed_errors/M-2.npy  \n",
            "  inflating: data/2018-05-19_15.00.10/smoothed_errors/M-3.npy  \n",
            "  inflating: data/2018-05-19_15.00.10/smoothed_errors/M-4.npy  \n",
            "  inflating: data/2018-05-19_15.00.10/smoothed_errors/M-5.npy  \n",
            "  inflating: data/2018-05-19_15.00.10/smoothed_errors/M-6.npy  \n",
            "  inflating: data/2018-05-19_15.00.10/smoothed_errors/M-7.npy  \n",
            "  inflating: data/2018-05-19_15.00.10/smoothed_errors/P-1.npy  \n",
            "  inflating: data/2018-05-19_15.00.10/smoothed_errors/P-10.npy  \n",
            "  inflating: data/2018-05-19_15.00.10/smoothed_errors/P-11.npy  \n",
            "  inflating: data/2018-05-19_15.00.10/smoothed_errors/P-14.npy  \n",
            "  inflating: data/2018-05-19_15.00.10/smoothed_errors/P-15.npy  \n",
            "  inflating: data/2018-05-19_15.00.10/smoothed_errors/P-2.npy  \n",
            "  inflating: data/2018-05-19_15.00.10/smoothed_errors/P-3.npy  \n",
            "  inflating: data/2018-05-19_15.00.10/smoothed_errors/P-4.npy  \n",
            "  inflating: data/2018-05-19_15.00.10/smoothed_errors/P-7.npy  \n",
            "  inflating: data/2018-05-19_15.00.10/smoothed_errors/R-1.npy  \n",
            "  inflating: data/2018-05-19_15.00.10/smoothed_errors/S-1.npy  \n",
            "  inflating: data/2018-05-19_15.00.10/smoothed_errors/S-2.npy  \n",
            "  inflating: data/2018-05-19_15.00.10/smoothed_errors/T-1.npy  \n",
            "  inflating: data/2018-05-19_15.00.10/smoothed_errors/T-10.npy  \n",
            "  inflating: data/2018-05-19_15.00.10/smoothed_errors/T-12.npy  \n",
            "  inflating: data/2018-05-19_15.00.10/smoothed_errors/T-13.npy  \n",
            "  inflating: data/2018-05-19_15.00.10/smoothed_errors/T-2.npy  \n",
            "  inflating: data/2018-05-19_15.00.10/smoothed_errors/T-3.npy  \n",
            "  inflating: data/2018-05-19_15.00.10/smoothed_errors/T-4.npy  \n",
            "  inflating: data/2018-05-19_15.00.10/smoothed_errors/T-5.npy  \n",
            "  inflating: data/2018-05-19_15.00.10/smoothed_errors/T-8.npy  \n",
            "  inflating: data/2018-05-19_15.00.10/smoothed_errors/T-9.npy  \n",
            "   creating: data/2018-05-19_15.00.10/y_hat/\n",
            "  inflating: data/2018-05-19_15.00.10/y_hat/A-1.npy  \n",
            "  inflating: data/2018-05-19_15.00.10/y_hat/A-2.npy  \n",
            "  inflating: data/2018-05-19_15.00.10/y_hat/A-3.npy  \n",
            "  inflating: data/2018-05-19_15.00.10/y_hat/A-4.npy  \n",
            "  inflating: data/2018-05-19_15.00.10/y_hat/A-5.npy  \n",
            "  inflating: data/2018-05-19_15.00.10/y_hat/A-6.npy  \n",
            "  inflating: data/2018-05-19_15.00.10/y_hat/A-7.npy  \n",
            "  inflating: data/2018-05-19_15.00.10/y_hat/A-8.npy  \n",
            "  inflating: data/2018-05-19_15.00.10/y_hat/A-9.npy  \n",
            "  inflating: data/2018-05-19_15.00.10/y_hat/B-1.npy  \n",
            "  inflating: data/2018-05-19_15.00.10/y_hat/C-1.npy  \n",
            "  inflating: data/2018-05-19_15.00.10/y_hat/C-2.npy  \n",
            "  inflating: data/2018-05-19_15.00.10/y_hat/D-1.npy  \n",
            "  inflating: data/2018-05-19_15.00.10/y_hat/D-11.npy  \n",
            "  inflating: data/2018-05-19_15.00.10/y_hat/D-12.npy  \n",
            "  inflating: data/2018-05-19_15.00.10/y_hat/D-13.npy  \n",
            "  inflating: data/2018-05-19_15.00.10/y_hat/D-14.npy  \n",
            "  inflating: data/2018-05-19_15.00.10/y_hat/D-15.npy  \n",
            "  inflating: data/2018-05-19_15.00.10/y_hat/D-16.npy  \n",
            "  inflating: data/2018-05-19_15.00.10/y_hat/D-2.npy  \n",
            "  inflating: data/2018-05-19_15.00.10/y_hat/D-3.npy  \n",
            "  inflating: data/2018-05-19_15.00.10/y_hat/D-4.npy  \n",
            "  inflating: data/2018-05-19_15.00.10/y_hat/D-5.npy  \n",
            "  inflating: data/2018-05-19_15.00.10/y_hat/D-6.npy  \n",
            "  inflating: data/2018-05-19_15.00.10/y_hat/D-7.npy  \n",
            "  inflating: data/2018-05-19_15.00.10/y_hat/D-8.npy  \n",
            "  inflating: data/2018-05-19_15.00.10/y_hat/D-9.npy  \n",
            "  inflating: data/2018-05-19_15.00.10/y_hat/E-1.npy  \n",
            "  inflating: data/2018-05-19_15.00.10/y_hat/E-10.npy  \n",
            "  inflating: data/2018-05-19_15.00.10/y_hat/E-11.npy  \n",
            "  inflating: data/2018-05-19_15.00.10/y_hat/E-12.npy  \n",
            "  inflating: data/2018-05-19_15.00.10/y_hat/E-13.npy  \n",
            "  inflating: data/2018-05-19_15.00.10/y_hat/E-2.npy  \n",
            "  inflating: data/2018-05-19_15.00.10/y_hat/E-3.npy  \n",
            "  inflating: data/2018-05-19_15.00.10/y_hat/E-4.npy  \n",
            "  inflating: data/2018-05-19_15.00.10/y_hat/E-5.npy  \n",
            "  inflating: data/2018-05-19_15.00.10/y_hat/E-6.npy  \n",
            "  inflating: data/2018-05-19_15.00.10/y_hat/E-7.npy  \n",
            "  inflating: data/2018-05-19_15.00.10/y_hat/E-8.npy  \n",
            "  inflating: data/2018-05-19_15.00.10/y_hat/E-9.npy  \n",
            "  inflating: data/2018-05-19_15.00.10/y_hat/F-1.npy  \n",
            "  inflating: data/2018-05-19_15.00.10/y_hat/F-2.npy  \n",
            "  inflating: data/2018-05-19_15.00.10/y_hat/F-3.npy  \n",
            "  inflating: data/2018-05-19_15.00.10/y_hat/F-4.npy  \n",
            "  inflating: data/2018-05-19_15.00.10/y_hat/F-5.npy  \n",
            "  inflating: data/2018-05-19_15.00.10/y_hat/F-7.npy  \n",
            "  inflating: data/2018-05-19_15.00.10/y_hat/F-8.npy  \n",
            "  inflating: data/2018-05-19_15.00.10/y_hat/G-1.npy  \n",
            "  inflating: data/2018-05-19_15.00.10/y_hat/G-2.npy  \n",
            "  inflating: data/2018-05-19_15.00.10/y_hat/G-3.npy  \n",
            "  inflating: data/2018-05-19_15.00.10/y_hat/G-4.npy  \n",
            "  inflating: data/2018-05-19_15.00.10/y_hat/G-6.npy  \n",
            "  inflating: data/2018-05-19_15.00.10/y_hat/G-7.npy  \n",
            "  inflating: data/2018-05-19_15.00.10/y_hat/M-1.npy  \n",
            "  inflating: data/2018-05-19_15.00.10/y_hat/M-2.npy  \n",
            "  inflating: data/2018-05-19_15.00.10/y_hat/M-3.npy  \n",
            "  inflating: data/2018-05-19_15.00.10/y_hat/M-4.npy  \n",
            "  inflating: data/2018-05-19_15.00.10/y_hat/M-5.npy  \n",
            "  inflating: data/2018-05-19_15.00.10/y_hat/M-6.npy  \n",
            "  inflating: data/2018-05-19_15.00.10/y_hat/M-7.npy  \n",
            "  inflating: data/2018-05-19_15.00.10/y_hat/P-1.npy  \n",
            "  inflating: data/2018-05-19_15.00.10/y_hat/P-10.npy  \n",
            "  inflating: data/2018-05-19_15.00.10/y_hat/P-11.npy  \n",
            "  inflating: data/2018-05-19_15.00.10/y_hat/P-14.npy  \n",
            "  inflating: data/2018-05-19_15.00.10/y_hat/P-15.npy  \n",
            "  inflating: data/2018-05-19_15.00.10/y_hat/P-2.npy  \n",
            "  inflating: data/2018-05-19_15.00.10/y_hat/P-3.npy  \n",
            "  inflating: data/2018-05-19_15.00.10/y_hat/P-4.npy  \n",
            "  inflating: data/2018-05-19_15.00.10/y_hat/P-7.npy  \n",
            "  inflating: data/2018-05-19_15.00.10/y_hat/R-1.npy  \n",
            "  inflating: data/2018-05-19_15.00.10/y_hat/S-1.npy  \n",
            "  inflating: data/2018-05-19_15.00.10/y_hat/S-2.npy  \n",
            "  inflating: data/2018-05-19_15.00.10/y_hat/T-1.npy  \n",
            "  inflating: data/2018-05-19_15.00.10/y_hat/T-10.npy  \n",
            "  inflating: data/2018-05-19_15.00.10/y_hat/T-12.npy  \n",
            "  inflating: data/2018-05-19_15.00.10/y_hat/T-13.npy  \n",
            "  inflating: data/2018-05-19_15.00.10/y_hat/T-2.npy  \n",
            "  inflating: data/2018-05-19_15.00.10/y_hat/T-3.npy  \n",
            "  inflating: data/2018-05-19_15.00.10/y_hat/T-4.npy  \n",
            "  inflating: data/2018-05-19_15.00.10/y_hat/T-5.npy  \n",
            "  inflating: data/2018-05-19_15.00.10/y_hat/T-8.npy  \n",
            "  inflating: data/2018-05-19_15.00.10/y_hat/T-9.npy  \n",
            "   creating: data/test/\n",
            "  inflating: data/test/A-1.npy       \n",
            "  inflating: data/test/A-2.npy       \n",
            "  inflating: data/test/A-3.npy       \n",
            "  inflating: data/test/A-4.npy       \n",
            "  inflating: data/test/A-5.npy       \n",
            "  inflating: data/test/A-6.npy       \n",
            "  inflating: data/test/A-7.npy       \n",
            "  inflating: data/test/A-8.npy       \n",
            "  inflating: data/test/A-9.npy       \n",
            "  inflating: data/test/B-1.npy       \n",
            "  inflating: data/test/C-1.npy       \n",
            "  inflating: data/test/C-2.npy       \n",
            "  inflating: data/test/D-1.npy       \n",
            "  inflating: data/test/D-11.npy      \n",
            "  inflating: data/test/D-12.npy      \n",
            "  inflating: data/test/D-13.npy      \n",
            "  inflating: data/test/D-14.npy      \n",
            "  inflating: data/test/D-15.npy      \n",
            "  inflating: data/test/D-16.npy      \n",
            "  inflating: data/test/D-2.npy       \n",
            "  inflating: data/test/D-3.npy       \n",
            "  inflating: data/test/D-4.npy       \n",
            "  inflating: data/test/D-5.npy       \n",
            "  inflating: data/test/D-6.npy       \n",
            "  inflating: data/test/D-7.npy       \n",
            "  inflating: data/test/D-8.npy       \n",
            "  inflating: data/test/D-9.npy       \n",
            "  inflating: data/test/E-1.npy       \n",
            "  inflating: data/test/E-10.npy      \n",
            "  inflating: data/test/E-11.npy      \n",
            "  inflating: data/test/E-12.npy      \n",
            "  inflating: data/test/E-13.npy      \n",
            "  inflating: data/test/E-2.npy       \n",
            "  inflating: data/test/E-3.npy       \n",
            "  inflating: data/test/E-4.npy       \n",
            "  inflating: data/test/E-5.npy       \n",
            "  inflating: data/test/E-6.npy       \n",
            "  inflating: data/test/E-7.npy       \n",
            "  inflating: data/test/E-8.npy       \n",
            "  inflating: data/test/E-9.npy       \n",
            "  inflating: data/test/F-1.npy       \n",
            "  inflating: data/test/F-2.npy       \n",
            "  inflating: data/test/F-3.npy       \n",
            "  inflating: data/test/F-4.npy       \n",
            "  inflating: data/test/F-5.npy       \n",
            "  inflating: data/test/F-7.npy       \n",
            "  inflating: data/test/F-8.npy       \n",
            "  inflating: data/test/G-1.npy       \n",
            "  inflating: data/test/G-2.npy       \n",
            "  inflating: data/test/G-3.npy       \n",
            "  inflating: data/test/G-4.npy       \n",
            "  inflating: data/test/G-6.npy       \n",
            "  inflating: data/test/G-7.npy       \n",
            "  inflating: data/test/M-1.npy       \n",
            "  inflating: data/test/M-2.npy       \n",
            "  inflating: data/test/M-3.npy       \n",
            "  inflating: data/test/M-4.npy       \n",
            "  inflating: data/test/M-5.npy       \n",
            "  inflating: data/test/M-6.npy       \n",
            "  inflating: data/test/M-7.npy       \n",
            "  inflating: data/test/P-1.npy       \n",
            "  inflating: data/test/P-10.npy      \n",
            "  inflating: data/test/P-11.npy      \n",
            "  inflating: data/test/P-14.npy      \n",
            "  inflating: data/test/P-15.npy      \n",
            "  inflating: data/test/P-2.npy       \n",
            "  inflating: data/test/P-3.npy       \n",
            "  inflating: data/test/P-4.npy       \n",
            "  inflating: data/test/P-7.npy       \n",
            "  inflating: data/test/R-1.npy       \n",
            "  inflating: data/test/S-1.npy       \n",
            "  inflating: data/test/S-2.npy       \n",
            "  inflating: data/test/T-1.npy       \n",
            "  inflating: data/test/T-10.npy      \n",
            "  inflating: data/test/T-12.npy      \n",
            "  inflating: data/test/T-13.npy      \n",
            "  inflating: data/test/T-2.npy       \n",
            "  inflating: data/test/T-3.npy       \n",
            "  inflating: data/test/T-4.npy       \n",
            "  inflating: data/test/T-5.npy       \n",
            "  inflating: data/test/T-8.npy       \n",
            "  inflating: data/test/T-9.npy       \n",
            "   creating: data/train/\n",
            "  inflating: data/train/A-1.npy      \n",
            "  inflating: data/train/A-2.npy      \n",
            "  inflating: data/train/A-3.npy      \n",
            "  inflating: data/train/A-4.npy      \n",
            "  inflating: data/train/A-5.npy      \n",
            "  inflating: data/train/A-6.npy      \n",
            "  inflating: data/train/A-7.npy      \n",
            "  inflating: data/train/A-8.npy      \n",
            "  inflating: data/train/A-9.npy      \n",
            "  inflating: data/train/B-1.npy      \n",
            "  inflating: data/train/C-1.npy      \n",
            "  inflating: data/train/C-2.npy      \n",
            "  inflating: data/train/D-1.npy      \n",
            "  inflating: data/train/D-11.npy     \n",
            "  inflating: data/train/D-12.npy     \n",
            "  inflating: data/train/D-13.npy     \n",
            "  inflating: data/train/D-14.npy     \n",
            "  inflating: data/train/D-15.npy     \n",
            "  inflating: data/train/D-16.npy     \n",
            "  inflating: data/train/D-2.npy      \n",
            "  inflating: data/train/D-3.npy      \n",
            "  inflating: data/train/D-4.npy      \n",
            "  inflating: data/train/D-5.npy      \n",
            "  inflating: data/train/D-6.npy      \n",
            "  inflating: data/train/D-7.npy      \n",
            "  inflating: data/train/D-8.npy      \n",
            "  inflating: data/train/D-9.npy      \n",
            "  inflating: data/train/E-1.npy      \n",
            "  inflating: data/train/E-10.npy     \n",
            "  inflating: data/train/E-11.npy     \n",
            "  inflating: data/train/E-12.npy     \n",
            "  inflating: data/train/E-13.npy     \n",
            "  inflating: data/train/E-2.npy      \n",
            "  inflating: data/train/E-3.npy      \n",
            "  inflating: data/train/E-4.npy      \n",
            "  inflating: data/train/E-5.npy      \n",
            "  inflating: data/train/E-6.npy      \n",
            "  inflating: data/train/E-7.npy      \n",
            "  inflating: data/train/E-8.npy      \n",
            "  inflating: data/train/E-9.npy      \n",
            "  inflating: data/train/F-1.npy      \n",
            "  inflating: data/train/F-2.npy      \n",
            "  inflating: data/train/F-3.npy      \n",
            "  inflating: data/train/F-4.npy      \n",
            "  inflating: data/train/F-5.npy      \n",
            "  inflating: data/train/F-7.npy      \n",
            "  inflating: data/train/F-8.npy      \n",
            "  inflating: data/train/G-1.npy      \n",
            "  inflating: data/train/G-2.npy      \n",
            "  inflating: data/train/G-3.npy      \n",
            "  inflating: data/train/G-4.npy      \n",
            "  inflating: data/train/G-6.npy      \n",
            "  inflating: data/train/G-7.npy      \n",
            "  inflating: data/train/M-1.npy      \n",
            "  inflating: data/train/M-2.npy      \n",
            "  inflating: data/train/M-3.npy      \n",
            "  inflating: data/train/M-4.npy      \n",
            "  inflating: data/train/M-5.npy      \n",
            "  inflating: data/train/M-6.npy      \n",
            "  inflating: data/train/M-7.npy      \n",
            "  inflating: data/train/P-1.npy      \n",
            "  inflating: data/train/P-10.npy     \n",
            "  inflating: data/train/P-11.npy     \n",
            "  inflating: data/train/P-14.npy     \n",
            "  inflating: data/train/P-15.npy     \n",
            "  inflating: data/train/P-2.npy      \n",
            "  inflating: data/train/P-3.npy      \n",
            "  inflating: data/train/P-4.npy      \n",
            "  inflating: data/train/P-7.npy      \n",
            "  inflating: data/train/R-1.npy      \n",
            "  inflating: data/train/S-1.npy      \n",
            "  inflating: data/train/S-2.npy      \n",
            "  inflating: data/train/T-1.npy      \n",
            "  inflating: data/train/T-10.npy     \n",
            "  inflating: data/train/T-12.npy     \n",
            "  inflating: data/train/T-13.npy     \n",
            "  inflating: data/train/T-2.npy      \n",
            "  inflating: data/train/T-3.npy      \n",
            "  inflating: data/train/T-4.npy      \n",
            "  inflating: data/train/T-5.npy      \n",
            "  inflating: data/train/T-8.npy      \n",
            "  inflating: data/train/T-9.npy      \n",
            "--2023-01-09 08:10:56--  https://raw.githubusercontent.com/khundman/telemanom/master/labeled_anomalies.csv\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 3956 (3.9K) [text/plain]\n",
            "Saving to: ‘labeled_anomalies.csv’\n",
            "\n",
            "labeled_anomalies.c 100%[===================>]   3.86K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-01-09 08:10:56 (31.5 MB/s) - ‘labeled_anomalies.csv’ saved [3956/3956]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def counting_data(path, list_, data_detail):\n",
        "    sum = 0\n",
        "    for i in list_:\n",
        "        np_file = np.load(path + i)\n",
        "        sum += np_file.shape[0]\n",
        "    print(f\"{data_detail} : {sum}\")\n",
        "    print(f\"Number of columns : {np_file.shape[1]}\")\n",
        "    return sum\n",
        "\n",
        "def counting_label(path, label_list, data_detail):\n",
        "    label_sum = 0\n",
        "    for i in label_list:\n",
        "        np_label = np.load(path + i)\n",
        "        sums = np.sum(np_label, axis=0)\n",
        "        label_sum += sums[0]\n",
        "    print(f\"{data_detail} : {label_sum}\")\n",
        "    return label_sum\n",
        "\n",
        "def counting_smd(path, list_, data_detail):\n",
        "    sum = 0\n",
        "    for i in list_:\n",
        "        np_file = np.load(path + i)\n",
        "        sum += np.sum(np_file.shape[0])\n",
        "    print(f\"{data_detail} : {sum}\")\n",
        "    print(f\"Number of columns : {np_file.shape[1]}\")\n",
        "    return sum\n",
        "\n",
        "def counting_smd_label(path, list_, data_detail):\n",
        "    sum = 0\n",
        "    for i in list_:\n",
        "        np_file = np.load(path + i)\n",
        "        sum += np.sum(np_file)\n",
        "    print(f\"{data_detail} : {sum}\")\n",
        "    return sum\n",
        "\n",
        "def counting_wadi(path, list_, data_detail):\n",
        "    sum = 0 \n",
        "    for i in list_:\n",
        "        np_file = np.load(path + i, allow_pickle=True)\n",
        "        sum += np.sum(np_file.shape[0])\n",
        "    print(f\"{data_detail} : {sum}\")\n",
        "    print(f\"Number of columns : {np_file.shape[1]}\")\n",
        "    return sum\n",
        "\n",
        "def counting_wadi_label(path, list_, data_detail):\n",
        "    sum = 0 \n",
        "    for i in list_:\n",
        "        np_file = np.load(path + i, allow_pickle=True).squeeze()\n",
        "        sum += np.sum(np_file)\n",
        "    print(f\"{data_detail} : {sum}\")\n",
        "    return sum"
      ],
      "metadata": {
        "id": "Mtg94jQRX3uj"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#-------------#\n",
        "#---# MSL #---#\n",
        "#-------------#\n",
        "MSL_list = os.listdir(\"/content/data/MSL\")\n",
        "train_list = [i for i in MSL_list if \"train\" in i]\n",
        "test_list = [i for i in MSL_list if \"test\" in i]\n",
        "label_list = [i for i in MSL_list if \"labels\" in i]\n",
        "\n",
        "train_num = counting_data(\"/content/data/MSL/\", train_list, \"Total number of MSL train data\")\n",
        "test_num = counting_data(\"/content/data/MSL/\", test_list, \"Total number of MSL test data\")\n",
        "label_num = counting_label(\"/content/data/MSL/\", label_list, \"Total number of MSL anomaly data\")\n",
        "\n",
        "print(f\"Percentage of anomaly\", label_num/test_num)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9G_tKbOBTIAP",
        "outputId": "fe301827-1b14-429a-9d23-1714c90059c7"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total number of MSL train data : 58317\n",
            "Number of columns : 55\n",
            "Total number of MSL test data : 73729\n",
            "Number of columns : 55\n",
            "Total number of MSL anomaly data : 7730.0\n",
            "Percentage of anomaly 0.10484341303964519\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#--------------#\n",
        "#---# SMAP #---#\n",
        "#--------------#\n",
        "smap_list = os.listdir(\"/content/data/SMAP\")\n",
        "train_list = [i for i in smap_list if \"train\" in i]\n",
        "test_list = [i for i in smap_list if \"test\" in i]\n",
        "label_list = [i for i in smap_list if \"labels\" in i]\n",
        "\n",
        "train_num = counting_data(\"/content/data/SMAP/\", train_list, \"Total number of SMAP train data\")\n",
        "test_num = counting_data(\"/content/data/SMAP/\", test_list, \"Total number of SMAP test data\")\n",
        "label_num = counting_label(\"/content/data/SMAP/\", label_list, \"Total number of SMAP anomaly data\")\n",
        "\n",
        "print(f\"Percentage of anomaly\", label_num/test_num)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HVviS--FVenG",
        "outputId": "118847e9-d7f3-46b5-abbd-886cca31eb1e"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total number of SMAP train data : 138004\n",
            "Number of columns : 25\n",
            "Total number of SMAP test data : 435826\n",
            "Number of columns : 25\n",
            "Total number of SMAP anomaly data : 55854.0\n",
            "Percentage of anomaly 0.12815664967211685\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "label_csv = pd.read_csv(\"/content/original_data/SMAP_MSL/labeled_anomalies.csv\")\n",
        "\n",
        "label_csv[label_csv.chan_id == 'T-10'] # no data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49
        },
        "id": "Y6SbJppyrKO0",
        "outputId": "6ead0ca7-ded2-426b-f501-4949a464daa0"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Empty DataFrame\n",
              "Columns: [chan_id, spacecraft, anomaly_sequences, class, num_values]\n",
              "Index: []"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-208603f7-7d86-4592-97fe-9ce914acdf22\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>chan_id</th>\n",
              "      <th>spacecraft</th>\n",
              "      <th>anomaly_sequences</th>\n",
              "      <th>class</th>\n",
              "      <th>num_values</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-208603f7-7d86-4592-97fe-9ce914acdf22')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-208603f7-7d86-4592-97fe-9ce914acdf22 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-208603f7-7d86-4592-97fe-9ce914acdf22');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "label_csv[label_csv.chan_id == 'P-2'] # Duplication of data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 112
        },
        "id": "v0uhdMT0S-67",
        "outputId": "06db4190-a757-4917-b5fc-27fc4d717d27"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   chan_id spacecraft anomaly_sequences    class  num_values\n",
              "17     P-2       SMAP    [[5350, 6575]]  [point]        8209\n",
              "51     P-2       SMAP    [[5300, 6420]]  [point]        8209"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-5dcb12cb-87ec-4991-8ca9-ed26d6a6bc0f\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>chan_id</th>\n",
              "      <th>spacecraft</th>\n",
              "      <th>anomaly_sequences</th>\n",
              "      <th>class</th>\n",
              "      <th>num_values</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>P-2</td>\n",
              "      <td>SMAP</td>\n",
              "      <td>[[5350, 6575]]</td>\n",
              "      <td>[point]</td>\n",
              "      <td>8209</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>51</th>\n",
              "      <td>P-2</td>\n",
              "      <td>SMAP</td>\n",
              "      <td>[[5300, 6420]]</td>\n",
              "      <td>[point]</td>\n",
              "      <td>8209</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-5dcb12cb-87ec-4991-8ca9-ed26d6a6bc0f')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-5dcb12cb-87ec-4991-8ca9-ed26d6a6bc0f button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-5dcb12cb-87ec-4991-8ca9-ed26d6a6bc0f');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## NAB"
      ],
      "metadata": {
        "id": "7O3epkm_izR1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = 'NAB'\n",
        "!git clone https://github.com/numenta/NAB.git\n",
        "shutil.move('/content/NAB/data', '/content/original_data/NAB')\n",
        "shutil.move('/content/NAB/labels', '/content/original_data/NAB')\n",
        "shutil.rmtree('/content/NAB')\n",
        "prepare_data(dataset)"
      ],
      "metadata": {
        "id": "80y80Wt4evGb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f320f3bf-9b20-4a19-ec4b-3b5083b504d7"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'NAB'...\n",
            "remote: Enumerating objects: 7029, done.\u001b[K\n",
            "remote: Counting objects: 100% (114/114), done.\u001b[K\n",
            "remote: Compressing objects: 100% (98/98), done.\u001b[K\n",
            "remote: Total 7029 (delta 48), reused 38 (delta 15), pack-reused 6915\u001b[K\n",
            "Receiving objects: 100% (7029/7029), 86.75 MiB | 22.58 MiB/s, done.\n",
            "Resolving deltas: 100% (4922/4922), done.\n",
            "Checking out files: 100% (1186/1186), done.\n",
            "ec2_request_latency_system_failure.csv shape (4032, 2)\n",
            "nyc_taxi.csv shape (10320, 2)\n",
            "cpu_utilization_asg_misconfiguration.csv shape (18050, 2)\n",
            "rogue_agent_key_updown.csv shape (5315, 2)\n",
            "rogue_agent_key_hold.csv shape (1882, 2)\n",
            "ambient_temperature_system_failure.csv shape (7267, 2)\n",
            "machine_temperature_system_failure.csv shape (22695, 2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nab_list = os.listdir(\"/content/data/NAB\")\n",
        "train_list = [i for i in nab_list if 'train' in i]\n",
        "test_list = [i for i in nab_list if 'test' in i]\n",
        "label_list = [i for i in nab_list if 'labels' in i]\n",
        "\n",
        "train_num = counting_data(\"/content/data/NAB/\", train_list, \"Total number of NAB train data\")\n",
        "test_num = counting_data(\"/content/data/NAB/\", test_list, \"Total number of NAB test data\")\n",
        "label_num = counting_label(\"/content/data/NAB/\", label_list, \"Total number of NAB anomaly data\")\n",
        "\n",
        "print(f\"Percentage of anomaly\", label_num/test_num)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y3Pp8QyTsFxm",
        "outputId": "12b715eb-6eec-444a-cd5d-c9a6db49e15a"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total number of NAB train data : 69561\n",
            "Number of columns : 1\n",
            "Total number of NAB test data : 69561\n",
            "Number of columns : 1\n",
            "Total number of NAB anomaly data : 6575.0\n",
            "Percentage of anomaly 0.09452135535716853\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## SMD"
      ],
      "metadata": {
        "id": "It7i9TzI-2Ff"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = 'SMD'\n",
        "!git clone https://github.com/NetManAIOps/OmniAnomaly.git\n",
        "shutil.move('/content/OmniAnomaly/ServerMachineDataset', '/content/original_data/SMD')\n",
        "shutil.rmtree('/content/OmniAnomaly')\n",
        "prepare_data(dataset)"
      ],
      "metadata": {
        "id": "zHb0Gxntfhhk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6a5ba7f7-a808-4042-dfc2-ca29d7f05518"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'OmniAnomaly'...\n",
            "remote: Enumerating objects: 204, done.\u001b[K\n",
            "remote: Counting objects: 100% (6/6), done.\u001b[K\n",
            "remote: Compressing objects: 100% (6/6), done.\u001b[K\n",
            "remote: Total 204 (delta 2), reused 0 (delta 0), pack-reused 198\u001b[K\n",
            "Receiving objects: 100% (204/204), 107.11 MiB | 22.08 MiB/s, done.\n",
            "Resolving deltas: 100% (80/80), done.\n",
            "Checking out files: 100% (132/132), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "smd_list = os.listdir(\"/content/data/SMD/\")\n",
        "train_list = [i for i in smd_list if 'train' in i]\n",
        "label_list = [i for i in smd_list if ('label' in i) and ('interpret' not in i) ]\n",
        "test_list = [i for i in smd_list if 'test' in i]\n",
        "\n",
        "\n",
        "train_num = counting_smd(\"/content/data/SMD/\", train_list, \"Total number of SMD train data\")\n",
        "test_num = counting_smd(\"/content/data/SMD/\", test_list, \"Total number of SMD test data\")\n",
        "label_num = counting_smd_label(\"/content/data/SMD/\", label_list, \"Total number of SMD anomaly data\")\n",
        "\n",
        "print(f\"Percentage of anomaly\", label_num/test_num)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DGZ_5JE-W5SA",
        "outputId": "a08e7f1d-f762-45dc-8790-9d58a0b6ef7c"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total number of SMD train data : 708405\n",
            "Number of columns : 38\n",
            "Total number of SMD test data : 708405\n",
            "Number of columns : 38\n",
            "Total number of SMD anomaly data : 29444.0\n",
            "Percentage of anomaly 0.04156379472194578\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##WADI\n"
      ],
      "metadata": {
        "id": "pQG5UkQ5ktsk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = 'WADI'\n",
        "# you can download https://itrust.sutd.edu.sg/itrust-labs_datasets/.\n",
        "# Download the data and upload it to Google Drive. You can put the path there here.\n",
        "dataset_folder = '/content/drive/MyDrive/WADI/WADI.A2_19 Nov 2019' ## your path!\n",
        "prepare_data(dataset, dataset_folder=dataset_folder)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ttR4aklkktYt",
        "outputId": "424ac7f1-6324-448e-daa4-8495f49a21e1"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train shape:(784571, 122), test shape :(172801, 122), label shape:(172801, 1)\n",
            "Exclude 9/29/2017 data\n",
            "Exclude 10/2/17 data\n",
            "Exclude 10/7/17 data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pd.read_csv(\"/content/drive/MyDrive/WADI/WADI.A2_19 Nov 2019/WADI_14days_new.csv\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 488
        },
        "id": "Zsc-5L14knCm",
        "outputId": "1b973b6a-4c0b-420c-ea6a-1032be84d03d"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "            Row       Date     Time  1_AIT_001_PV  1_AIT_002_PV  1_AIT_003_PV  \\\n",
              "0             1  9/25/2017  00:00.0       171.155      0.619473       11.5759   \n",
              "1             2  9/25/2017  00:01.0       171.155      0.619473       11.5759   \n",
              "2             3  9/25/2017  00:02.0       171.155      0.619473       11.5759   \n",
              "3             4  9/25/2017  00:03.0       171.155      0.607477       11.5725   \n",
              "4             5  9/25/2017  00:04.0       171.155      0.607477       11.5725   \n",
              "...         ...        ...      ...           ...           ...           ...   \n",
              "784566  1048567    10/7/17  16:06.0       175.855      0.589478       11.8941   \n",
              "784567  1048568    10/7/17  16:07.0       175.855      0.589478       11.8941   \n",
              "784568  1048569    10/7/17  16:08.0       175.855      0.589478       11.8941   \n",
              "784569  1048570    10/7/17  16:09.0       175.896      0.613476       11.8913   \n",
              "784570  1048571    10/7/17  16:10.0       175.896      0.613476       11.8913   \n",
              "\n",
              "        1_AIT_004_PV  1_AIT_005_PV  1_FIT_001_PV  1_LS_001_AL  ...  \\\n",
              "0            504.645      0.318319      0.001157            0  ...   \n",
              "1            504.645      0.318319      0.001157            0  ...   \n",
              "2            504.645      0.318319      0.001157            0  ...   \n",
              "3            504.673      0.318438      0.001207            0  ...   \n",
              "4            504.673      0.318438      0.001207            0  ...   \n",
              "...              ...           ...           ...          ...  ...   \n",
              "784566       479.191      0.331571      0.001128            0  ...   \n",
              "784567       479.191      0.331571      0.001128            0  ...   \n",
              "784568       479.191      0.331571      0.001128            0  ...   \n",
              "784569       479.224      0.331622      0.001173            0  ...   \n",
              "784570       479.224      0.331622      0.001173            0  ...   \n",
              "\n",
              "        3_MV_001_STATUS  3_MV_002_STATUS  3_MV_003_STATUS  3_P_001_STATUS  \\\n",
              "0                     1                1                1               1   \n",
              "1                     1                1                1               1   \n",
              "2                     1                1                1               1   \n",
              "3                     1                1                1               1   \n",
              "4                     1                1                1               1   \n",
              "...                 ...              ...              ...             ...   \n",
              "784566                1                1                1               1   \n",
              "784567                1                1                1               1   \n",
              "784568                1                1                1               1   \n",
              "784569                1                1                1               1   \n",
              "784570                1                1                1               1   \n",
              "\n",
              "        3_P_002_STATUS  3_P_003_STATUS  3_P_004_STATUS  LEAK_DIFF_PRESSURE  \\\n",
              "0                    1               1               1             67.9651   \n",
              "1                    1               1               1             67.9651   \n",
              "2                    1               1               1             67.9651   \n",
              "3                    1               1               1             67.1948   \n",
              "4                    1               1               1             67.1948   \n",
              "...                ...             ...             ...                 ...   \n",
              "784566               1               1               1             60.6305   \n",
              "784567               1               1               1             60.6305   \n",
              "784568               1               1               1             60.6305   \n",
              "784569               1               1               1             60.4477   \n",
              "784570               1               1               1             60.4477   \n",
              "\n",
              "        PLANT_START_STOP_LOG  TOTAL_CONS_REQUIRED_FLOW  \n",
              "0                          1                      0.68  \n",
              "1                          1                      0.68  \n",
              "2                          1                      0.68  \n",
              "3                          1                      0.68  \n",
              "4                          1                      0.68  \n",
              "...                      ...                       ...  \n",
              "784566                     1                      0.25  \n",
              "784567                     1                      0.25  \n",
              "784568                     1                      0.25  \n",
              "784569                     1                      0.25  \n",
              "784570                     1                      0.25  \n",
              "\n",
              "[784571 rows x 130 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-05257d6c-1cf2-4770-9d0f-9a2d1d360e24\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Row</th>\n",
              "      <th>Date</th>\n",
              "      <th>Time</th>\n",
              "      <th>1_AIT_001_PV</th>\n",
              "      <th>1_AIT_002_PV</th>\n",
              "      <th>1_AIT_003_PV</th>\n",
              "      <th>1_AIT_004_PV</th>\n",
              "      <th>1_AIT_005_PV</th>\n",
              "      <th>1_FIT_001_PV</th>\n",
              "      <th>1_LS_001_AL</th>\n",
              "      <th>...</th>\n",
              "      <th>3_MV_001_STATUS</th>\n",
              "      <th>3_MV_002_STATUS</th>\n",
              "      <th>3_MV_003_STATUS</th>\n",
              "      <th>3_P_001_STATUS</th>\n",
              "      <th>3_P_002_STATUS</th>\n",
              "      <th>3_P_003_STATUS</th>\n",
              "      <th>3_P_004_STATUS</th>\n",
              "      <th>LEAK_DIFF_PRESSURE</th>\n",
              "      <th>PLANT_START_STOP_LOG</th>\n",
              "      <th>TOTAL_CONS_REQUIRED_FLOW</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>9/25/2017</td>\n",
              "      <td>00:00.0</td>\n",
              "      <td>171.155</td>\n",
              "      <td>0.619473</td>\n",
              "      <td>11.5759</td>\n",
              "      <td>504.645</td>\n",
              "      <td>0.318319</td>\n",
              "      <td>0.001157</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>67.9651</td>\n",
              "      <td>1</td>\n",
              "      <td>0.68</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>9/25/2017</td>\n",
              "      <td>00:01.0</td>\n",
              "      <td>171.155</td>\n",
              "      <td>0.619473</td>\n",
              "      <td>11.5759</td>\n",
              "      <td>504.645</td>\n",
              "      <td>0.318319</td>\n",
              "      <td>0.001157</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>67.9651</td>\n",
              "      <td>1</td>\n",
              "      <td>0.68</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>9/25/2017</td>\n",
              "      <td>00:02.0</td>\n",
              "      <td>171.155</td>\n",
              "      <td>0.619473</td>\n",
              "      <td>11.5759</td>\n",
              "      <td>504.645</td>\n",
              "      <td>0.318319</td>\n",
              "      <td>0.001157</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>67.9651</td>\n",
              "      <td>1</td>\n",
              "      <td>0.68</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>9/25/2017</td>\n",
              "      <td>00:03.0</td>\n",
              "      <td>171.155</td>\n",
              "      <td>0.607477</td>\n",
              "      <td>11.5725</td>\n",
              "      <td>504.673</td>\n",
              "      <td>0.318438</td>\n",
              "      <td>0.001207</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>67.1948</td>\n",
              "      <td>1</td>\n",
              "      <td>0.68</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>9/25/2017</td>\n",
              "      <td>00:04.0</td>\n",
              "      <td>171.155</td>\n",
              "      <td>0.607477</td>\n",
              "      <td>11.5725</td>\n",
              "      <td>504.673</td>\n",
              "      <td>0.318438</td>\n",
              "      <td>0.001207</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>67.1948</td>\n",
              "      <td>1</td>\n",
              "      <td>0.68</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>784566</th>\n",
              "      <td>1048567</td>\n",
              "      <td>10/7/17</td>\n",
              "      <td>16:06.0</td>\n",
              "      <td>175.855</td>\n",
              "      <td>0.589478</td>\n",
              "      <td>11.8941</td>\n",
              "      <td>479.191</td>\n",
              "      <td>0.331571</td>\n",
              "      <td>0.001128</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>60.6305</td>\n",
              "      <td>1</td>\n",
              "      <td>0.25</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>784567</th>\n",
              "      <td>1048568</td>\n",
              "      <td>10/7/17</td>\n",
              "      <td>16:07.0</td>\n",
              "      <td>175.855</td>\n",
              "      <td>0.589478</td>\n",
              "      <td>11.8941</td>\n",
              "      <td>479.191</td>\n",
              "      <td>0.331571</td>\n",
              "      <td>0.001128</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>60.6305</td>\n",
              "      <td>1</td>\n",
              "      <td>0.25</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>784568</th>\n",
              "      <td>1048569</td>\n",
              "      <td>10/7/17</td>\n",
              "      <td>16:08.0</td>\n",
              "      <td>175.855</td>\n",
              "      <td>0.589478</td>\n",
              "      <td>11.8941</td>\n",
              "      <td>479.191</td>\n",
              "      <td>0.331571</td>\n",
              "      <td>0.001128</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>60.6305</td>\n",
              "      <td>1</td>\n",
              "      <td>0.25</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>784569</th>\n",
              "      <td>1048570</td>\n",
              "      <td>10/7/17</td>\n",
              "      <td>16:09.0</td>\n",
              "      <td>175.896</td>\n",
              "      <td>0.613476</td>\n",
              "      <td>11.8913</td>\n",
              "      <td>479.224</td>\n",
              "      <td>0.331622</td>\n",
              "      <td>0.001173</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>60.4477</td>\n",
              "      <td>1</td>\n",
              "      <td>0.25</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>784570</th>\n",
              "      <td>1048571</td>\n",
              "      <td>10/7/17</td>\n",
              "      <td>16:10.0</td>\n",
              "      <td>175.896</td>\n",
              "      <td>0.613476</td>\n",
              "      <td>11.8913</td>\n",
              "      <td>479.224</td>\n",
              "      <td>0.331622</td>\n",
              "      <td>0.001173</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>60.4477</td>\n",
              "      <td>1</td>\n",
              "      <td>0.25</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>784571 rows × 130 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-05257d6c-1cf2-4770-9d0f-9a2d1d360e24')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-05257d6c-1cf2-4770-9d0f-9a2d1d360e24 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-05257d6c-1cf2-4770-9d0f-9a2d1d360e24');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "smd_list = os.listdir(\"/content/data/WADI/\")\n",
        "train_list = [i for i in smd_list if 'train' in i]\n",
        "label_list = [i for i in smd_list if 'labels' in i]\n",
        "test_list = [i for i in smd_list if 'test' in i]\n",
        "\n",
        "train_num = counting_wadi(\"/content/data/WADI/\", train_list, \"Total number of WADI train data\")\n",
        "test_num = counting_wadi(\"/content/data/WADI/\", test_list, \"Total number of WADI test data\")\n",
        "label_num = counting_wadi_label(\"/content/data/WADI/\", label_list, \"Total number of WADI anomaly data\")\n",
        "\n",
        "print(f\"Percentage of anomaly\", label_num/test_num)"
      ],
      "metadata": {
        "id": "rCJzic33w322",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fa4c5869-065a-42b4-d149-b812dfe343f5"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total number of WADI train data : 626400\n",
            "Number of columns : 121\n",
            "Total number of WADI test data : 172801\n",
            "Number of columns : 121\n",
            "Total number of WADI anomaly data : 9977\n",
            "Percentage of anomaly 0.05773693439274078\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "gS3aSSbRm4EN"
      },
      "execution_count": 17,
      "outputs": []
    }
  ]
}